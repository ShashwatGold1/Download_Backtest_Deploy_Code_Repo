{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0b112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- Show All rows & columns -----------------------------\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373fbb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- play_sound -----------------------------\n",
    "\n",
    "# import winsound\n",
    "# import os\n",
    "\n",
    "# import sys; sys.path.append('..')\n",
    "# from config import MYPATHS\n",
    "\n",
    "# def play_sound(file_name, error_message, raise_error):\n",
    "\n",
    "#     # Alert_01, Alert_02, Atma_rama_Alarm\n",
    "#     sound_path = os.path.join(MYPATHS['Audio_files'], f\"{file_name}.wav\")\n",
    "\n",
    "#     if not os.path.isfile(sound_path):\n",
    "#         print(f\"\\033[1;31mSound file not found: {sound_path}\\033[0m\")\n",
    "#         if raise_error: raise FileNotFoundError(f\"Missing sound file: {sound_path}\")\n",
    "#         return\n",
    "\n",
    "#     winsound.PlaySound(sound_path, winsound.SND_FILENAME | winsound.SND_ASYNC)\n",
    "#     print(f\"\\033[1;31m{error_message}\\033[0m\")\n",
    "\n",
    "#     if raise_error:\n",
    "#         raise Exception(f\"\\033[1;31m{error_message}\\033[0m\")\n",
    "\n",
    "# play_sound(file_name = \"Atma_rama_Alarm\", error_message = \"error_message\", raise_error = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1318afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- Get 1 minute data with retry -----------------------------\n",
    "\n",
    "# import datetime\n",
    "# import pandas as pd\n",
    "# import time\n",
    "\n",
    "# def get_data(security_id):\n",
    "    \n",
    "#     today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#     for i in range(0, 3):\n",
    "#         # Get the latest minute chart data\n",
    "#         minute_chart = dhan.intraday_minute_data(\n",
    "#             security_id=security_id,\n",
    "#             exchange_segment=\"NSE_FNO\", \n",
    "#             instrument_type=\"OPTIDX\",\n",
    "#             from_date=today,\n",
    "#             to_date=today, \n",
    "#             interval=1\n",
    "#         )\n",
    "\n",
    "#         # Check if data is None or empty\n",
    "#         if not minute_chart[\"data\"]:\n",
    "#             print(\"Your API subscription has expired or API related error\", security_id)\n",
    "#             continue\n",
    "\n",
    "#         # Convert the data to a DataFrame\n",
    "#         minute_chart_df = pd.DataFrame(minute_chart[\"data\"])\n",
    "#         minute_chart_df['datetime'] = pd.to_datetime(minute_chart_df['timestamp'], unit='s') + pd.Timedelta(hours=5, minutes=30)\n",
    "#         minute_chart_df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "#         if minute_chart_df is not None:\n",
    "#             return minute_chart_df\n",
    "        \n",
    "#         time.sleep(0.14)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62e72b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- Get index of price filter effeciently from the dataframe -----------------------------\n",
    "\n",
    "# def fast_select_three_rows(df, price_col):\n",
    "#     # Convert to NumPy array for speed\n",
    "#     prices = df[price_col].values\n",
    "#     idx_below = (prices < 270).nonzero()[0]\n",
    "    \n",
    "#     if len(idx_below) == 0:\n",
    "#         raise ValueError(f\"No {price_col} < 270 found.\")\n",
    "    \n",
    "#     # Get index of max price below 270\n",
    "#     middle_pos = idx_below[prices[idx_below].argmax()]\n",
    "\n",
    "#     # Collect surrounding indices\n",
    "#     start = max(0, middle_pos - 1)\n",
    "#     end = min(len(df), middle_pos + 2)\n",
    "\n",
    "#     return df.iloc[start:end]\n",
    "\n",
    "# # Usage:\n",
    "# ce_selected = fast_select_three_rows(df, 'ce_last_price').copy()\n",
    "# ce_selected.drop(columns=['pe_last_price', 'pe_iv', 'pe_oi', 'pe_volume', 'security_id_PE'], inplace=True)\n",
    "\n",
    "# pe_selected = fast_select_three_rows(df, 'pe_last_price').copy()\n",
    "# pe_selected.drop(columns=['security_id_CE', 'ce_iv', 'ce_volume', 'ce_oi', 'ce_last_price'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "802fe398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- Tick counts of folder's file -----------------------------\n",
    "\n",
    "# # Calculate total tick count for all files in the specified folder using read_all_ticks\n",
    "# import os\n",
    "# import sys; sys.path.append('..'); from config import MYPATHS\n",
    "\n",
    "# folder_path = MYPATHS['Simulation_Tick_Data'] + f\"\\\\market_data_2025-06-26\"\n",
    "# total_tick_count = 0\n",
    "\n",
    "# for file_name in os.listdir(folder_path):\n",
    "#     if file_name.endswith(\".bin\"):\n",
    "#         security_id = file_name.split(\".\")[0]\n",
    "#         ticks = read_all_ticks(security_id, data_path=MYPATHS['base'], simulation_mode=\"Simulation\")\n",
    "#         total_tick_count += len(ticks)\n",
    "#         print(f\"Security ID: {security_id}, Tick Count: {len(ticks)}\")\n",
    "\n",
    "# print(f\"Total tick count in folder: {total_tick_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99ee1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- Check Internet Connection -----------------------------\n",
    "\n",
    "# from datetime import datetime, timedelta\n",
    "# import socket\n",
    "# from IPython.display import display, HTML, Javascript\n",
    "# import time\n",
    "\n",
    "# def update_element(s):\n",
    "#     display(HTML(\"\"\"<div><pre id=\"clock\" style='margin:0; padding:0;'></pre></div>\"\"\"))\n",
    "#     display(Javascript(f\"\"\"document.getElementById('clock').innerHTML = '{s}';\"\"\"))\n",
    "\n",
    "# def is_internet_available():\n",
    "\n",
    "#     starting_time = str(datetime.now())\n",
    "#     start_time = datetime.strptime(starting_time, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "#     time_elapsed = None\n",
    "\n",
    "#     while True:\n",
    "#         try:\n",
    "#             # Attempt to connect to Google's DNS server\n",
    "#             socket.create_connection((\"8.8.8.8\", 53), timeout=3)\n",
    "#             if time_elapsed is not None:\n",
    "#                 print(f\"Successfully connected to the internet after 0{time_elapsed}\")\n",
    "#             print(f\"Internet Connection: 'Available'\")\n",
    "#             return True\n",
    "#         except OSError:\n",
    "            \n",
    "#             current_time = datetime.now()\n",
    "#             time_elapsed = str(current_time - start_time).split('.')[0]\n",
    "\n",
    "#             update_element(f\"Internet connection lost, retrying... Time Elapsed: 0{time_elapsed}\")\n",
    "\n",
    "#             time.sleep(1)\n",
    "\n",
    "# is_internet_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "355998f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- Read Tick data from binary file -----------------------------\n",
    "\n",
    "# from pathlib import Path\n",
    "# import struct\n",
    "# from datetime import datetime\n",
    "# import sys; sys.path.append('..'); from config import MYPATHS\n",
    "\n",
    "# class TickFileReader:\n",
    "#     \"\"\"Fast reader for tick files - supports parallel reading\"\"\"\n",
    "    \n",
    "#     RECORD_SIZE = 32\n",
    "\n",
    "#     @staticmethod\n",
    "#     def read_full_file(filepath):\n",
    "#         \"\"\"Read entire file - returns all ticks\"\"\"\n",
    "#         ticks = []\n",
    "#         try:\n",
    "#             with open(filepath, 'rb') as f_file:\n",
    "#                 while True:\n",
    "#                     data = f_file.read(TickFileReader.RECORD_SIZE)\n",
    "#                     if len(data) != TickFileReader.RECORD_SIZE:\n",
    "#                         break\n",
    "                    \n",
    "#                     time_bytes, price, oi, volume = struct.unpack('8sddd', data)\n",
    "#                     ltt = time_bytes.rstrip(b'\\x00').decode('utf-8')\n",
    "                    \n",
    "#                     if ltt:\n",
    "#                         ticks.append({\n",
    "#                             'LTT': ltt,\n",
    "#                             'price': price,\n",
    "#                             'OI': oi,\n",
    "#                             'volume': volume\n",
    "#                         })\n",
    "#         except Exception as e:\n",
    "#             print(f\"âŒ Error reading full file: {e}\")\n",
    "        \n",
    "#         return ticks\n",
    "\n",
    "# def read_all_ticks(folder_path, file_name):\n",
    "#     \"\"\"Read all ticks for a security\"\"\"\n",
    "#     return TickFileReader.read_full_file(Path(folder_path) / f\"{file_name}.bin\")\n",
    "\n",
    "# date = \"2025-06-26\"\n",
    "# folder_path = Path(MYPATHS['base']) / \"Simulation_Tick_Data\" / f\"market_data_{date}\"\n",
    "\n",
    "# all_ticks = read_all_ticks(folder_path=folder_path, file_name=\"62371\")\n",
    "# print(f\"Total ticks: {len(all_ticks)}\")\n",
    "# all_ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53969aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- Convert .txt to .bin and .bin to .txt for raw simulation data-----------------------------\n",
    "\n",
    "# import ast\n",
    "# import pickle\n",
    "# import struct\n",
    "# from pathlib import Path\n",
    "# import sys; sys.path.append('..'); from config import MYPATHS\n",
    "\n",
    "\n",
    "# class SimulationDataConverter:\n",
    "#     \"\"\"Convert between simulation_data.txt and simulation_data.bin formats\"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "    \n",
    "#     def convert_txt_to_bin(self, input_txt_path, output_bin_path=None):\n",
    "#         \"\"\"\n",
    "#         Convert .txt simulation data to .bin format\n",
    "        \n",
    "#         Args:\n",
    "#             input_txt_path: Path to input .txt file\n",
    "#             output_bin_path: Path to output .bin file (optional, auto-generated if None)\n",
    "#         \"\"\"\n",
    "#         input_path = Path(input_txt_path)\n",
    "        \n",
    "#         # Validate input file\n",
    "#         if not input_path.exists():\n",
    "#             raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "        \n",
    "#         if not input_path.suffix.lower() == '.txt':\n",
    "#             raise ValueError(f\"Input file must be .txt format, got: {input_path.suffix}\")\n",
    "        \n",
    "#         # Generate output path if not provided\n",
    "#         if output_bin_path is None:\n",
    "#             output_bin_path = input_path.with_suffix('.bin')\n",
    "#         else:\n",
    "#             output_bin_path = Path(output_bin_path)\n",
    "        \n",
    "#         # Ensure output directory exists\n",
    "#         output_bin_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         print(f\"ðŸ“„ Converting: {input_path.name}\")\n",
    "#         print(f\"ðŸ“ Input:  {input_path}\")\n",
    "#         print(f\"ðŸ“ Output: {output_bin_path}\")\n",
    "        \n",
    "#         converted_count = 0\n",
    "        \n",
    "#         try:\n",
    "#             with open(input_path, 'r') as txt_file, open(output_bin_path, 'wb') as bin_file:\n",
    "#                 for line_num, line in enumerate(txt_file, 1):\n",
    "#                     line = line.strip()\n",
    "#                     if not line:\n",
    "#                         continue\n",
    "                    \n",
    "#                     try:\n",
    "#                         # Parse the dictionary string (same as simulator)\n",
    "#                         tick_dict = ast.literal_eval(line)\n",
    "                        \n",
    "#                         # Convert to binary using pickle (same format as live data)\n",
    "#                         binary_data = pickle.dumps(tick_dict)\n",
    "                        \n",
    "#                         # Write length prefix + data (same format as live data)\n",
    "#                         bin_file.write(struct.pack('I', len(binary_data)))\n",
    "#                         bin_file.write(binary_data)\n",
    "                        \n",
    "#                         converted_count += 1\n",
    "                        \n",
    "#                         # Progress indicator for large files\n",
    "#                         if converted_count % 100000 == 0:\n",
    "#                             print(f\"ðŸ“Š Converted {converted_count} records...\")\n",
    "                            \n",
    "#                     except (ValueError, SyntaxError) as e:\n",
    "#                         print(f\"âš ï¸ Skipping malformed line {line_num}: {e}\")\n",
    "#                         continue\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"âŒ Error processing line {line_num}: {e}\")\n",
    "#                         continue\n",
    "            \n",
    "#             print(f\"âœ… Conversion completed!\")\n",
    "#             print(f\"ðŸ“Š Total records converted: {converted_count}\")\n",
    "#             print(f\"ðŸ’¾ Output file size: {output_bin_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "            \n",
    "#             return {\n",
    "#                 'input_file': str(input_path),\n",
    "#                 'output_file': str(output_bin_path),\n",
    "#                 'records_converted': converted_count,\n",
    "#                 'output_size_mb': output_bin_path.stat().st_size / (1024*1024)\n",
    "#             }\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"âŒ Conversion failed: {e}\")\n",
    "#             # Clean up partial file on error\n",
    "#             if output_bin_path.exists():\n",
    "#                 output_bin_path.unlink()\n",
    "#             raise\n",
    "    \n",
    "#     def convert_bin_to_txt(self, input_bin_path, output_txt_path=None):\n",
    "#         \"\"\"\n",
    "#         Convert .bin simulation data to .txt format\n",
    "        \n",
    "#         Args:\n",
    "#             input_bin_path: Path to input .bin file\n",
    "#             output_txt_path: Path to output .txt file (optional, auto-generated if None)\n",
    "#         \"\"\"\n",
    "#         input_path = Path(input_bin_path)\n",
    "        \n",
    "#         # Validate input file\n",
    "#         if not input_path.exists():\n",
    "#             raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "        \n",
    "#         if not input_path.suffix.lower() == '.bin':\n",
    "#             raise ValueError(f\"Input file must be .bin format, got: {input_path.suffix}\")\n",
    "        \n",
    "#         # Generate output path if not provided\n",
    "#         if output_txt_path is None:\n",
    "#             output_txt_path = input_path.with_suffix('.txt')\n",
    "#         else:\n",
    "#             output_txt_path = Path(output_txt_path)\n",
    "        \n",
    "#         # Ensure output directory exists\n",
    "#         output_txt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         print(f\"ðŸ“„ Converting: {input_path.name}\")\n",
    "#         print(f\"ðŸ“ Input:  {input_path}\")\n",
    "#         print(f\"ðŸ“ Output: {output_txt_path}\")\n",
    "        \n",
    "#         converted_count = 0\n",
    "        \n",
    "#         try:\n",
    "#             with open(input_path, 'rb') as bin_file, open(output_txt_path, 'w') as txt_file:\n",
    "#                 while True:\n",
    "#                     try:\n",
    "#                         # Read length prefix (4 bytes)\n",
    "#                         length_data = bin_file.read(4)\n",
    "#                         if len(length_data) != 4:\n",
    "#                             break  # End of file\n",
    "                        \n",
    "#                         # Unpack length\n",
    "#                         data_length = struct.unpack('I', length_data)[0]\n",
    "                        \n",
    "#                         # Read the pickled data\n",
    "#                         pickled_data = bin_file.read(data_length)\n",
    "#                         if len(pickled_data) != data_length:\n",
    "#                             raise ValueError(f\"Binary file corrupted: Expected {data_length} bytes, got {len(pickled_data)}\")\n",
    "                        \n",
    "#                         # Unpickle the dictionary\n",
    "#                         tick_dict = pickle.loads(pickled_data)\n",
    "                        \n",
    "#                         # Write as dictionary string (same format as original .txt)\n",
    "#                         txt_file.write(str(tick_dict) + '\\n')\n",
    "                        \n",
    "#                         converted_count += 1\n",
    "                        \n",
    "#                         # Progress indicator for large files\n",
    "#                         if converted_count % 100000 == 0:\n",
    "#                             print(f\"ðŸ“Š Converted {converted_count} records...\")\n",
    "                            \n",
    "#                     except pickle.PickleError as e:\n",
    "#                         print(f\"âŒ Pickle error at record {converted_count + 1}: {e}\")\n",
    "#                         break\n",
    "#                     except struct.error as e:\n",
    "#                         print(f\"âŒ Structure error at record {converted_count + 1}: {e}\")\n",
    "#                         break\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"âŒ Error at record {converted_count + 1}: {e}\")\n",
    "#                         break\n",
    "            \n",
    "#             print(f\"âœ… Conversion completed!\")\n",
    "#             print(f\"ðŸ“Š Total records converted: {converted_count}\")\n",
    "#             print(f\"ðŸ’¾ Output file size: {output_txt_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "            \n",
    "#             return {\n",
    "#                 'input_file': str(input_path),\n",
    "#                 'output_file': str(output_txt_path),\n",
    "#                 'records_converted': converted_count,\n",
    "#                 'output_size_mb': output_txt_path.stat().st_size / (1024*1024)\n",
    "#             }\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"âŒ Conversion failed: {e}\")\n",
    "#             # Clean up partial file on error\n",
    "#             if output_txt_path.exists():\n",
    "#                 output_txt_path.unlink()\n",
    "#             raise\n",
    "    \n",
    "#     def batch_convert_folder(self, folder_path, output_folder=None, convert_to='bin'):\n",
    "#         \"\"\"\n",
    "#         Convert all files in a folder between .txt and .bin formats\n",
    "        \n",
    "#         Args:\n",
    "#             folder_path: Path to folder containing files\n",
    "#             output_folder: Output folder (optional, uses same folder if None)\n",
    "#             convert_to: 'bin' to convert .txt to .bin, 'txt' to convert .bin to .txt\n",
    "#         \"\"\"\n",
    "#         folder = Path(folder_path)\n",
    "        \n",
    "#         if not folder.exists() or not folder.is_dir():\n",
    "#             raise ValueError(f\"Invalid folder path: {folder}\")\n",
    "        \n",
    "#         # Find files based on conversion direction\n",
    "#         if convert_to == 'bin':\n",
    "#             source_files = list(folder.glob(\"*.txt\"))\n",
    "#             source_ext = '.txt'\n",
    "#             target_ext = '.bin'\n",
    "#         elif convert_to == 'txt':\n",
    "#             source_files = list(folder.glob(\"*.bin\"))\n",
    "#             source_ext = '.bin'\n",
    "#             target_ext = '.txt'\n",
    "#         else:\n",
    "#             raise ValueError(\"convert_to must be 'bin' or 'txt'\")\n",
    "        \n",
    "#         if not source_files:\n",
    "#             print(f\"ðŸ“‚ No {source_ext} files found in: {folder}\")\n",
    "#             return []\n",
    "        \n",
    "#         print(f\"ðŸ“‚ Found {len(source_files)} {source_ext} files to convert to {target_ext}\")\n",
    "        \n",
    "#         results = []\n",
    "        \n",
    "#         for source_file in sorted(source_files):\n",
    "#             try:\n",
    "#                 if output_folder:\n",
    "#                     output_path = Path(output_folder) / source_file.with_suffix(target_ext).name\n",
    "#                 else:\n",
    "#                     output_path = source_file.with_suffix(target_ext)\n",
    "                \n",
    "#                 if convert_to == 'bin':\n",
    "#                     result = self.convert_txt_to_bin(source_file, output_path)\n",
    "#                 else:\n",
    "#                     result = self.convert_bin_to_txt(source_file, output_path)\n",
    "                \n",
    "#                 results.append(result)\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"âŒ Failed to convert {source_file.name}: {e}\")\n",
    "#                 continue\n",
    "        \n",
    "#         print(f\"ðŸŽ¯ Batch conversion completed: {len(results)}/{len(source_files)} files converted\")\n",
    "#         return results\n",
    "    \n",
    "#     def verify_conversion(self, original_path, converted_path):\n",
    "#         \"\"\"\n",
    "#         Verify that conversion was successful by comparing record counts\n",
    "        \n",
    "#         Args:\n",
    "#             original_path: Path to original file (.txt or .bin)\n",
    "#             converted_path: Path to converted file (.bin or .txt)\n",
    "#         \"\"\"\n",
    "#         print(f\"ðŸ” Verifying conversion...\")\n",
    "        \n",
    "#         original_path = Path(original_path)\n",
    "#         converted_path = Path(converted_path)\n",
    "        \n",
    "#         original_count = 0\n",
    "#         converted_count = 0\n",
    "        \n",
    "#         # Count records in original file\n",
    "#         if original_path.suffix.lower() == '.txt':\n",
    "#             with open(original_path, 'r') as f:\n",
    "#                 for line in f:\n",
    "#                     if line.strip():\n",
    "#                         original_count += 1\n",
    "        \n",
    "#         elif original_path.suffix.lower() == '.bin':\n",
    "#             with open(original_path, 'rb') as f:\n",
    "#                 while True:\n",
    "#                     try:\n",
    "#                         length_data = f.read(4)\n",
    "#                         if len(length_data) != 4:\n",
    "#                             break\n",
    "#                         data_length = struct.unpack('I', length_data)[0]\n",
    "#                         pickled_data = f.read(data_length)\n",
    "#                         if len(pickled_data) != data_length:\n",
    "#                             break\n",
    "#                         pickle.loads(pickled_data)\n",
    "#                         original_count += 1\n",
    "#                     except:\n",
    "#                         break\n",
    "        \n",
    "#         # Count records in converted file\n",
    "#         if converted_path.suffix.lower() == '.txt':\n",
    "#             with open(converted_path, 'r') as f:\n",
    "#                 for line in f:\n",
    "#                     if line.strip():\n",
    "#                         converted_count += 1\n",
    "        \n",
    "#         elif converted_path.suffix.lower() == '.bin':\n",
    "#             with open(converted_path, 'rb') as f:\n",
    "#                 while True:\n",
    "#                     try:\n",
    "#                         length_data = f.read(4)\n",
    "#                         if len(length_data) != 4:\n",
    "#                             break\n",
    "#                         data_length = struct.unpack('I', length_data)[0]\n",
    "#                         pickled_data = f.read(data_length)\n",
    "#                         if len(pickled_data) != data_length:\n",
    "#                             break\n",
    "#                         pickle.loads(pickled_data)\n",
    "#                         converted_count += 1\n",
    "#                     except:\n",
    "#                         break\n",
    "        \n",
    "#         if original_count == converted_count:\n",
    "#             print(f\"âœ… Verification successful: {converted_count} records match\")\n",
    "#             return True\n",
    "#         else:\n",
    "#             print(f\"âŒ Verification failed: Original has {original_count} records, Converted has {converted_count} records\")\n",
    "#             return False\n",
    "\n",
    "\n",
    "# # Convenience functions for easy usage\n",
    "# def convert_txt_to_bin(txt_file_path, bin_file_path=None):\n",
    "#     \"\"\"Convert single .txt file to .bin format\"\"\"\n",
    "#     converter = SimulationDataConverter()\n",
    "#     return converter.convert_txt_to_bin(txt_file_path, bin_file_path)\n",
    "\n",
    "# def convert_bin_to_txt(bin_file_path, txt_file_path=None):\n",
    "#     \"\"\"Convert single .bin file to .txt format\"\"\"\n",
    "#     converter = SimulationDataConverter()\n",
    "#     return converter.convert_bin_to_txt(bin_file_path, txt_file_path)\n",
    "\n",
    "# def convert_folder_to_bin(folder_path, output_folder=None):\n",
    "#     \"\"\"Convert all .txt files in folder to .bin format\"\"\"\n",
    "#     converter = SimulationDataConverter()\n",
    "#     return converter.batch_convert_folder(folder_path, output_folder, 'bin')\n",
    "\n",
    "# def convert_folder_to_txt(folder_path, output_folder=None):\n",
    "#     \"\"\"Convert all .bin files in folder to .txt format\"\"\"\n",
    "#     converter = SimulationDataConverter()\n",
    "#     return converter.batch_convert_folder(folder_path, output_folder, 'txt')\n",
    "\n",
    "# def verify_conversion(original_file, converted_file):\n",
    "#     \"\"\"Verify conversion was successful\"\"\"\n",
    "#     converter = SimulationDataConverter()\n",
    "#     return converter.verify_conversion(original_file, converted_file)\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     input_file_path = MYPATHS['Raw_data_bin'] + f\"\\\\2025-06-26.bin\"\n",
    "#     output_file_path = MYPATHS['Raw_data_txt'] + f\"\\\\2025-06-26.txt\"\n",
    "\n",
    "#     # # Convert .txt to .bin\n",
    "#     # convert_txt_to_bin(input_file_path, output_file_path)\n",
    "    \n",
    "#     # Convert .bin to .txt\n",
    "#     # convert_bin_to_txt(input_file_path, output_file_path)\n",
    "    \n",
    "#     # # Convert folder of .txt files to .bin\n",
    "#     # convert_folder_to_bin(r\"D:\\example\\txt_files\")\n",
    "    \n",
    "#     # # Convert folder of .bin files to .txt\n",
    "#     # convert_folder_to_txt(r\"D:\\example\\bin_files\")\n",
    "    \n",
    "#     # Verify conversion\n",
    "#     verify_conversion(input_file_path, output_file_path)\n",
    "    \n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc4b5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- Read Raw simulation data from binary file -----------------------------\n",
    "\n",
    "# import pickle\n",
    "# import struct\n",
    "# from pathlib import Path\n",
    "# import sys; sys.path.append('..'); from config import MYPATHS\n",
    "\n",
    "\n",
    "# class BinFileReader:\n",
    "#     \"\"\"Ultra-fast binary file reader for simulation data\"\"\"\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def read_bin_file(file_path):\n",
    "#         \"\"\"\n",
    "#         Read complete .bin file and return all tick data\n",
    "        \n",
    "#         Args:\n",
    "#             file_path: Path to .bin file\n",
    "            \n",
    "#         Returns:\n",
    "#             list: List of all tick dictionaries\n",
    "#         \"\"\"\n",
    "#         file_path = Path(file_path)\n",
    "        \n",
    "#         if not file_path.exists():\n",
    "#             raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "#         if not file_path.suffix.lower() == '.bin':\n",
    "#             raise ValueError(f\"File must be .bin format, got: {file_path.suffix}\")\n",
    "        \n",
    "#         ticks = []\n",
    "        \n",
    "#         with open(file_path, 'rb') as f:\n",
    "#             while True:\n",
    "#                 # Read length prefix (4 bytes)\n",
    "#                 length_data = f.read(4)\n",
    "#                 if len(length_data) != 4:\n",
    "#                     break  # End of file\n",
    "                \n",
    "#                 # Unpack length\n",
    "#                 data_length = struct.unpack('I', length_data)[0]\n",
    "                \n",
    "#                 # Read pickled data\n",
    "#                 pickled_data = f.read(data_length)\n",
    "#                 if len(pickled_data) != data_length:\n",
    "#                     raise ValueError(f\"Corrupted file: Expected {data_length} bytes, got {len(pickled_data)}\")\n",
    "                \n",
    "#                 # Unpickle and add to list\n",
    "#                 tick_dict = pickle.loads(pickled_data)\n",
    "#                 ticks.append(tick_dict)\n",
    "        \n",
    "#         return ticks\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def read_bin_file_generator(file_path):\n",
    "#         \"\"\"\n",
    "#         Memory-efficient generator to read .bin file one tick at a time\n",
    "        \n",
    "#         Args:\n",
    "#             file_path: Path to .bin file\n",
    "            \n",
    "#         Yields:\n",
    "#             dict: Individual tick dictionary\n",
    "#         \"\"\"\n",
    "#         file_path = Path(file_path)\n",
    "        \n",
    "#         if not file_path.exists():\n",
    "#             raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "#         with open(file_path, 'rb') as f:\n",
    "#             while True:\n",
    "#                 # Read length prefix\n",
    "#                 length_data = f.read(4)\n",
    "#                 if len(length_data) != 4:\n",
    "#                     break\n",
    "                \n",
    "#                 # Read and unpickle data\n",
    "#                 data_length = struct.unpack('I', length_data)[0]\n",
    "#                 pickled_data = f.read(data_length)\n",
    "                \n",
    "#                 if len(pickled_data) != data_length:\n",
    "#                     raise ValueError(f\"Corrupted file at position {f.tell()}\")\n",
    "                \n",
    "#                 yield pickle.loads(pickled_data)\n",
    "\n",
    "# # Convenience functions\n",
    "# def read_full_bin(file_path):\n",
    "#     \"\"\"Read complete .bin file - returns all ticks\"\"\"\n",
    "#     return BinFileReader.read_bin_file(file_path)\n",
    "\n",
    "# def read_bin_streaming(file_path):\n",
    "#     \"\"\"Memory-efficient streaming read - yields one tick at a time\"\"\"\n",
    "#     return BinFileReader.read_bin_file_generator(file_path)\n",
    "\n",
    "# # -------------------------------------\n",
    "\n",
    "# # Example usage\n",
    "# file_path = MYPATHS['Raw_data_bin'] + f\"\\\\2025-06-30.bin\"\n",
    "\n",
    "# # # # Method 1: Read all data at once (fast, uses more memory)\n",
    "# # all_ticks = read_full_bin(file_path)\n",
    "# # print(f\"Loaded {len(all_ticks)} ticks\")\n",
    "\n",
    "# # # Method 2: Stream data one by one (memory efficient)\n",
    "# # for i, tick in enumerate(read_bin_streaming(file_path)):\n",
    "# #     print(f\"Tick {i}: {tick}\")\n",
    "# #     # print(tick)\n",
    "# #     if i >= 10:\n",
    "# #         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accdf731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------- Preventing the Internet Connection Loss -----------------------------\n",
    "\n",
    "# import keyboard\n",
    "# import random\n",
    "# import time\n",
    "\n",
    "# def prevent_sleep_with_win_search(interval=150):\n",
    "\n",
    "#     # Sample search terms (you can expand this list)\n",
    "#     search_terms = [\n",
    "#         \"cmd\", \"notepad\", \"calc\", \"python\", \"edge\",\n",
    "#         \"paint\", \"settings\", \"task manager\", \"snipping tool\",\n",
    "#         \"control panel\", \"windows update\", \"event viewer\"\n",
    "#     ]\n",
    "\n",
    "#     try:\n",
    "#         while True:\n",
    "#             term = random.choice(search_terms)\n",
    "\n",
    "#             # Press Windows key\n",
    "#             keyboard.press_and_release('windows')\n",
    "#             time.sleep(0.5)\n",
    "\n",
    "#             # Type search term\n",
    "#             keyboard.write(term, delay=0.05)\n",
    "#             time.sleep(1)\n",
    "\n",
    "#             # Press Esc to close Start menu\n",
    "#             keyboard.press_and_release('esc')\n",
    "\n",
    "#             # Wait until next iteration\n",
    "#             time.sleep(interval)\n",
    "\n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"\\nStopped.\")\n",
    "\n",
    "# # Run it\n",
    "# prevent_sleep_with_win_search(interval=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "240c0a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured instrument 62402 with timeframes: ['1s', '5s', '1m', '3m']\n",
      "Processed 62402: 1 candles saved\n",
      "Processed 62402: 1 candles saved\n",
      "Processed 62402: 1 candles saved\n",
      "Processed 62402: 1 candles saved\n",
      "Configured instrument 62403 with timeframes: ['1s', '5s', '1m', '3m']\n",
      "Processed 62403: 2 candles saved\n",
      "\n",
      "Final stats: {'tick_count': 7, 'candle_count': 6, 'error_count': 0, 'instruments': 2, 'total_timeframes': 8, 'pending_writes': 0}\n",
      "Instrument 62402: Total 4 timeframes, completed: {'1s': 4, '5s': 0, '1m': 0, '3m': 0}\n",
      "Instrument 62403: Total 4 timeframes, completed: {'1s': 1, '5s': 1, '1m': 0, '3m': 0}\n",
      "Shutting down engine...\n",
      "Shutdown complete. Final stats: {'tick_count': 7, 'candle_count': 6, 'error_count': 0, 'instruments': 2, 'total_timeframes': 8, 'pending_writes': 0}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Callable, Union\n",
    "from collections import defaultdict, deque\n",
    "import logging\n",
    "import asyncio\n",
    "import aiofiles\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "@dataclass\n",
    "class Candle:\n",
    "    __slots__ = ['timestamp_str', 'open', 'high', 'low', 'close']\n",
    "    \n",
    "    timestamp_str: str  # \"10:50:15\" format\n",
    "    open: float\n",
    "    high: float\n",
    "    low: float\n",
    "    close: float\n",
    "\n",
    "class TimeframeEngine:\n",
    "    def __init__(self, interval_seconds: int):\n",
    "        self.interval_seconds = interval_seconds\n",
    "        self.current_candle = None\n",
    "        self.completed_candles = deque(maxlen=1000)\n",
    "        self.current_interval_start = None\n",
    "    \n",
    "    def process_tick(self, tick_data: dict) -> Optional[Candle]:\n",
    "        try:\n",
    "            price = float(tick_data['LTP'])\n",
    "            time_str = tick_data['LTT']\n",
    "            \n",
    "            interval_start_str = self._get_interval_start_str(time_str)\n",
    "            \n",
    "            completed_candle = None\n",
    "            \n",
    "            # Complete current candle if new interval\n",
    "            if self.current_candle and interval_start_str != self.current_interval_start:\n",
    "                completed_candle = self.current_candle\n",
    "                self.completed_candles.append(completed_candle)\n",
    "                self.current_candle = None\n",
    "            \n",
    "            # Create or update candle\n",
    "            if self.current_candle is None:\n",
    "                self.current_candle = Candle(\n",
    "                    timestamp_str=interval_start_str,\n",
    "                    open=price,\n",
    "                    high=price,\n",
    "                    low=price,\n",
    "                    close=price\n",
    "                )\n",
    "                self.current_interval_start = interval_start_str\n",
    "            else:\n",
    "                self.current_candle.high = max(self.current_candle.high, price)\n",
    "                self.current_candle.low = min(self.current_candle.low, price)\n",
    "                self.current_candle.close = price\n",
    "            \n",
    "            return completed_candle\n",
    "            \n",
    "        except (ValueError, KeyError, IndexError) as e:\n",
    "            logging.error(f\"Error processing tick: {e}, tick_data: {tick_data}\")\n",
    "            return None\n",
    "    \n",
    "    def _get_interval_start_str(self, time_str: str) -> str:\n",
    "        try:\n",
    "            time_parts = time_str.split(':')\n",
    "            if len(time_parts) != 3:\n",
    "                raise ValueError(f\"Invalid time format: {time_str}\")\n",
    "            \n",
    "            hours = int(time_parts[0])\n",
    "            minutes = int(time_parts[1])\n",
    "            seconds = int(time_parts[2])\n",
    "            \n",
    "            # Convert to seconds from market open (09:15:00)\n",
    "            market_open_seconds = 9 * 3600 + 15 * 60  # 09:15:00 in seconds\n",
    "            current_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "            \n",
    "            # Calculate elapsed seconds from market open\n",
    "            elapsed = current_seconds - market_open_seconds\n",
    "            if elapsed < 0:\n",
    "                elapsed = 0  # Before market open\n",
    "            \n",
    "            # Calculate interval number\n",
    "            interval_number = elapsed // self.interval_seconds\n",
    "            \n",
    "            # Calculate interval start\n",
    "            interval_start_seconds = market_open_seconds + (interval_number * self.interval_seconds)\n",
    "            \n",
    "            # Convert back to HH:MM:SS\n",
    "            start_hours = interval_start_seconds // 3600\n",
    "            start_minutes = (interval_start_seconds % 3600) // 60\n",
    "            start_secs = interval_start_seconds % 60\n",
    "            \n",
    "            return f\"{start_hours:02d}:{start_minutes:02d}:{start_secs:02d}\"\n",
    "            \n",
    "        except (ValueError, IndexError) as e:\n",
    "            logging.error(f\"Error parsing time {time_str}: {e}\")\n",
    "            return time_str  # Fallback to original time\n",
    "\n",
    "class AsyncCandleEngine:\n",
    "    def __init__(self, output_folder: str = None, default_timeframes: List[str] = None):\n",
    "        self.output_folder = output_folder\n",
    "        self.default_timeframes = default_timeframes or ['1s', '5s', '1m']\n",
    "        \n",
    "        # Per-instrument timeframes\n",
    "        self.instruments: Dict[int, Dict[str, TimeframeEngine]] = defaultdict(lambda: {})\n",
    "        self.configured_instruments = set()\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.tick_count = 0\n",
    "        self.candle_count = 0\n",
    "        self.error_count = 0\n",
    "        \n",
    "        # Callbacks\n",
    "        self.on_candle_complete: Optional[Callable] = None\n",
    "        self.on_error: Optional[Callable] = None\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.ERROR)\n",
    "        \n",
    "        # Create output folders\n",
    "        if output_folder:\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    def set_default_timeframes(self, timeframes: List[str]):\n",
    "        \"\"\"Set default timeframes for all instruments\"\"\"\n",
    "        self.default_timeframes = timeframes\n",
    "        print(f\"Default timeframes set: {timeframes}\")\n",
    "    \n",
    "    def configure_instrument(self, security_id: Union[int, List[int]], timeframes: List[str] = None):\n",
    "        \"\"\"Configure instrument(s) with timeframes\"\"\"\n",
    "        if timeframes is None:\n",
    "            timeframes = self.default_timeframes\n",
    "        \n",
    "        # Handle single security_id or list\n",
    "        security_ids = [security_id] if isinstance(security_id, int) else security_id\n",
    "        \n",
    "        for sid in security_ids:\n",
    "            try:\n",
    "                # Clear existing configuration\n",
    "                if sid in self.instruments:\n",
    "                    self.instruments[sid].clear()\n",
    "                \n",
    "                # Configure new timeframes\n",
    "                for tf_str in timeframes:\n",
    "                    interval_seconds = self._parse_timeframe(tf_str)\n",
    "                    tf_engine = TimeframeEngine(interval_seconds)\n",
    "                    self.instruments[sid][tf_str] = tf_engine\n",
    "                \n",
    "                self.configured_instruments.add(sid)\n",
    "                print(f\"Configured instrument {sid} with timeframes: {timeframes}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error configuring instrument {sid}: {e}\")\n",
    "                if self.on_error:\n",
    "                    self.on_error(f\"Configuration error for {sid}: {e}\")\n",
    "    \n",
    "    def configure_multiple_instruments(self, security_ids: List[int], timeframes: List[str] = None):\n",
    "        \"\"\"Configure multiple instruments at once\"\"\"\n",
    "        self.configure_instrument(security_ids, timeframes)\n",
    "    \n",
    "    def _parse_timeframe(self, tf_str: str) -> int:\n",
    "        \"\"\"Convert timeframe string to seconds\"\"\"\n",
    "        try:\n",
    "            tf_str = tf_str.lower().strip()\n",
    "            \n",
    "            if tf_str.endswith('s'):\n",
    "                return int(tf_str[:-1])\n",
    "            elif tf_str.endswith('m'):\n",
    "                return int(tf_str[:-1]) * 60\n",
    "            elif tf_str.endswith('h'):\n",
    "                return int(tf_str[:-1]) * 3600\n",
    "            elif tf_str.endswith('d'):\n",
    "                return int(tf_str[:-1]) * 86400\n",
    "            else:\n",
    "                return int(tf_str)\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"Invalid timeframe format: {tf_str}\")\n",
    "            raise ValueError(f\"Invalid timeframe format: {tf_str}\")\n",
    "    \n",
    "    async def process_tick(self, tick_data: dict) -> Dict[str, Candle]:\n",
    "        \"\"\"Process tick data with async instant file saving\"\"\"\n",
    "        try:\n",
    "            self.tick_count += 1\n",
    "            \n",
    "            # Validate tick data\n",
    "            if not self._validate_tick_data(tick_data):\n",
    "                self.error_count += 1\n",
    "                return {}\n",
    "            \n",
    "            security_id = int(tick_data['security_id'])\n",
    "            \n",
    "            # Auto-configure with defaults if not configured\n",
    "            if security_id not in self.configured_instruments:\n",
    "                self.configure_instrument(security_id)\n",
    "            \n",
    "            completed_candles = {}\n",
    "            write_tasks = []\n",
    "            \n",
    "            # Process tick for each timeframe\n",
    "            for tf_name, tf_engine in self.instruments[security_id].items():\n",
    "                completed_candle = tf_engine.process_tick(tick_data)\n",
    "                \n",
    "                if completed_candle:\n",
    "                    self.candle_count += 1\n",
    "                    completed_candles[tf_name] = completed_candle\n",
    "                    \n",
    "                    # INSTANT ASYNC SAVE - Create async write task\n",
    "                    if self.output_folder:\n",
    "                        task = self._write_candle_async(security_id, tf_name, completed_candle)\n",
    "                        write_tasks.append(task)\n",
    "                    \n",
    "                    # Trigger callback\n",
    "                    if self.on_candle_complete:\n",
    "                        try:\n",
    "                            self.on_candle_complete(security_id, tf_name, completed_candle)\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Error in candle callback: {e}\")\n",
    "            \n",
    "            # Wait for all writes to complete instantly\n",
    "            if write_tasks:\n",
    "                await asyncio.gather(*write_tasks)\n",
    "            \n",
    "            return completed_candles\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.error_count += 1\n",
    "            logging.error(f\"Error processing tick: {e}, tick_data: {tick_data}\")\n",
    "            if self.on_error:\n",
    "                self.on_error(f\"Tick processing error: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    async def process_batch(self, tick_batch: List[dict]) -> List[Dict[str, Candle]]:\n",
    "        \"\"\"Process batch of ticks with async instant saving\"\"\"\n",
    "        tasks = [self.process_tick(tick) for tick in tick_batch]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return [result for result in results if result]\n",
    "    \n",
    "    def _validate_tick_data(self, tick_data: dict) -> bool:\n",
    "        \"\"\"Validate tick data format\"\"\"\n",
    "        try:\n",
    "            required_fields = ['security_id', 'LTP', 'LTT']\n",
    "            \n",
    "            for field in required_fields:\n",
    "                if field not in tick_data:\n",
    "                    logging.error(f\"Missing required field: {field}\")\n",
    "                    return False\n",
    "            \n",
    "            # Validate security_id\n",
    "            int(tick_data['security_id'])\n",
    "            \n",
    "            # Validate LTP\n",
    "            float(tick_data['LTP'])\n",
    "            \n",
    "            # Validate LTT format\n",
    "            time_str = tick_data['LTT']\n",
    "            if not isinstance(time_str, str) or len(time_str.split(':')) != 3:\n",
    "                logging.error(f\"Invalid time format: {time_str}\")\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except (ValueError, TypeError) as e:\n",
    "            logging.error(f\"Tick validation error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    async def _write_candle_async(self, security_id: int, tf_name: str, candle: Candle):\n",
    "        \"\"\"Asynchronous instant file writing\"\"\"\n",
    "        try:\n",
    "            candle_dir = os.path.join(self.output_folder, tf_name)\n",
    "            os.makedirs(candle_dir, exist_ok=True)\n",
    "            \n",
    "            filename = os.path.join(candle_dir, f\"{security_id}_{tf_name}.csv\")\n",
    "            write_header = not os.path.exists(filename)\n",
    "            \n",
    "            # Use aiofiles for async file operations\n",
    "            async with aiofiles.open(filename, 'a', newline='') as f:\n",
    "                if write_header:\n",
    "                    await f.write('timestamp,open,high,low,close,volume\\n')\n",
    "                \n",
    "                line = f\"{candle.timestamp_str},{candle.open},{candle.high},{candle.low},{candle.close},0.0\\n\"\n",
    "                await f.write(line)\n",
    "                await f.flush()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error writing candle to file: {e}\")\n",
    "    \n",
    "    def get_current_candles(self, security_id: int) -> Dict[str, Candle]:\n",
    "        \"\"\"Get current incomplete candles for instrument\"\"\"\n",
    "        if security_id not in self.instruments:\n",
    "            return {}\n",
    "        \n",
    "        current_candles = {}\n",
    "        for tf_name, tf_engine in self.instruments[security_id].items():\n",
    "            if tf_engine.current_candle:\n",
    "                current_candles[tf_name] = tf_engine.current_candle\n",
    "        \n",
    "        return current_candles\n",
    "    \n",
    "    def get_completed_candles(self, security_id: int, tf_name: str, count: int = None) -> List[Candle]:\n",
    "        \"\"\"Get completed candles for specific timeframe\"\"\"\n",
    "        if security_id not in self.instruments or tf_name not in self.instruments[security_id]:\n",
    "            return []\n",
    "        \n",
    "        tf_engine = self.instruments[security_id][tf_name]\n",
    "        candles = list(tf_engine.completed_candles)\n",
    "        \n",
    "        if count:\n",
    "            return candles[-count:]\n",
    "        return candles\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get engine statistics\"\"\"\n",
    "        return {\n",
    "            'tick_count': self.tick_count,\n",
    "            'candle_count': self.candle_count,\n",
    "            'error_count': self.error_count,\n",
    "            'instruments': len(self.configured_instruments),\n",
    "            'total_timeframes': sum(len(tfs) for tfs in self.instruments.values()),\n",
    "            'pending_writes': 0  # Always 0 with async instant saving\n",
    "        }\n",
    "    \n",
    "    def get_configured_instruments(self) -> List[int]:\n",
    "        \"\"\"Get list of configured instruments\"\"\"\n",
    "        return list(self.configured_instruments)\n",
    "    \n",
    "    def get_instrument_timeframes(self, security_id: int) -> List[str]:\n",
    "        \"\"\"Get timeframes for specific instrument\"\"\"\n",
    "        if security_id in self.instruments:\n",
    "            return list(self.instruments[security_id].keys())\n",
    "        return []\n",
    "    \n",
    "    def set_candle_callback(self, callback: Callable):\n",
    "        \"\"\"Set callback for candle completion\"\"\"\n",
    "        self.on_candle_complete = callback\n",
    "    \n",
    "    def set_error_callback(self, callback: Callable):\n",
    "        \"\"\"Set callback for errors\"\"\"\n",
    "        self.on_error = callback\n",
    "    \n",
    "    async def shutdown(self):\n",
    "        \"\"\"Graceful shutdown - write any remaining current candles\"\"\"\n",
    "        print(\"Shutting down engine...\")\n",
    "        \n",
    "        # Write current incomplete candles\n",
    "        if self.output_folder:\n",
    "            write_tasks = []\n",
    "            for security_id, timeframes in self.instruments.items():\n",
    "                for tf_name, tf_engine in timeframes.items():\n",
    "                    if tf_engine.current_candle:\n",
    "                        task = self._write_candle_async(security_id, tf_name, tf_engine.current_candle)\n",
    "                        write_tasks.append(task)\n",
    "            \n",
    "            if write_tasks:\n",
    "                await asyncio.gather(*write_tasks)\n",
    "        \n",
    "        final_stats = self.get_stats()\n",
    "        print(f\"Shutdown complete. Final stats: {final_stats}\")\n",
    "\n",
    "# -----------------------------------------\n",
    "\n",
    "# Usage examples\n",
    "async def async_usage_example():\n",
    "    \"\"\"Pure async usage example\"\"\"\n",
    "    \n",
    "    # Initialize engine\n",
    "    engine = AsyncCandleEngine(\n",
    "        output_folder=\"./async_candles\",\n",
    "        default_timeframes=['1s', '5s', '1m', '3m']\n",
    "    )\n",
    "    \n",
    "    # # Configure instruments \n",
    "    # engine.configure_instrument(62403, ['5s', '15s', '10m'])\n",
    "    \n",
    "    \n",
    "    # # Set callback\n",
    "    # def on_candle_complete(security_id, tf_name, candle):\n",
    "    #     print(f\"[ASYNC SAVED] {security_id} {tf_name}: {candle.timestamp_str} \"\n",
    "    #           f\"OHLC=({candle.open:.2f},{candle.high:.2f},{candle.low:.2f},{candle.close:.2f})\")\n",
    "    \n",
    "    # engine.set_candle_callback(on_candle_complete)\n",
    "    \n",
    "    # Sample tick data\n",
    "    ticks = [\n",
    "        {'security_id': 62402, 'LTP': '1600.00', 'LTT': '09:15:00'},\n",
    "        {'security_id': 62402, 'LTP': '1600.50', 'LTT': '09:15:01'},  # 1s candle saved\n",
    "        {'security_id': 62402, 'LTP': '1599.75', 'LTT': '09:15:02'},  # 1s candle saved\n",
    "        {'security_id': 62402, 'LTP': '1601.00', 'LTT': '09:15:03'},  # 1s candle saved\n",
    "        {'security_id': 62402, 'LTP': '1600.25', 'LTT': '09:15:04'},  # 1s + 4s candles saved\n",
    "        {'security_id': 62403, 'LTP': '2800.50', 'LTT': '09:15:01'},  # 1s candle saved\n",
    "        {'security_id': 62403, 'LTP': '2799.75', 'LTT': '09:15:05'},  # 1s + 5s candles saved\n",
    "    ]\n",
    "        \n",
    "    # Method 1: Process one by one\n",
    "    for tick in ticks:\n",
    "        completed_candles = await engine.process_tick(tick)\n",
    "        if completed_candles:\n",
    "            print(f\"Processed {tick['security_id']}: {len(completed_candles)} candles saved\")\n",
    "    \n",
    "    # # Method 2: Process batch (parallel)\n",
    "    # batch_results = await engine.process_batch(ticks)\n",
    "    # print(f\"Batch processed: {len(batch_results)} results\")\n",
    "    \n",
    "    # Show stats\n",
    "    stats = engine.get_stats()\n",
    "    print(f\"\\nFinal stats: {stats}\")\n",
    "    \n",
    "    # Show timeframes states\n",
    "    for security_id in engine.get_configured_instruments():\n",
    "        timeframes = engine.get_current_candles(security_id)\n",
    "        completed = {tf: len(engine.get_completed_candles(security_id, tf)) \n",
    "                    for tf in engine.get_instrument_timeframes(security_id)}\n",
    "        print(f\"Instrument {security_id}: Total {len(timeframes)} timeframes, completed: {completed}\")\n",
    "    \n",
    "    # Shutdown\n",
    "    await engine.shutdown()\n",
    "        \n",
    "asyncio.run(async_usage_example())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
