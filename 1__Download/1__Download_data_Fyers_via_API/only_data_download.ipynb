{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from fyers_apiv3 import fyersModel\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import time\n",
    "\n",
    "client_id = \"UZJ1B1WF4N-100\"\n",
    "secret_key = \"KJ7CW6C2I8\"\n",
    "redirect_uri = \"https://www.google.com/\"\n",
    "response_type = \"code\"  \n",
    "state = \"sample_state\"\n",
    "grant_type = \"authorization_code\"  \n",
    "totp_secret = \"SFJM4CZGYCQZFK75K3AAMCKBPHNMABQ4\"\n",
    "pin = \"5555\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "session = fyersModel.SessionModel(\n",
    "    client_id=client_id,\n",
    "    secret_key=secret_key,\n",
    "    redirect_uri=redirect_uri,\n",
    "    response_type=response_type\n",
    ")\n",
    "\n",
    "response = session.generate_authcode()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_code = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhcHBfaWQiOiJVWkoxQjFXRjROIiwidXVpZCI6Ijk2MjQwMjdhNzUxMjQzNmQ5YTE4NTI0NGIzNjJlZDMzIiwiaXBBZGRyIjoiIiwibm9uY2UiOiIiLCJzY29wZSI6IiIsImRpc3BsYXlfbmFtZSI6IllLMDg5NzMiLCJvbXMiOiJLMSIsImhzbV9rZXkiOiI5YjRmM2VmMDZmYWJhNjIwN2E4MWQ4Y2RkNjUxZjVkMWI2ZDhmMDdkMjUyYjc4YjA0ZDYzYTYyNyIsImlzRGRwaUVuYWJsZWQiOiJZIiwiaXNNdGZFbmFibGVkIjoiTiIsImF1ZCI6IltcImQ6MVwiLFwiZDoyXCIsXCJ4OjBcIixcIng6MVwiLFwieDoyXCJdIiwiZXhwIjoxNzYwMDUzMDcwLCJpYXQiOjE3NjAwMjMwNzAsImlzcyI6ImFwaS5sb2dpbi5meWVycy5pbiIsIm5iZiI6MTc2MDAyMzA3MCwic3ViIjoiYXV0aF9jb2RlIn0._kETAvO_BayL0fn8akMGnI4x2RoMMN6l2K3YpaYHW_8\"\n",
    "\n",
    "session = fyersModel.SessionModel(\n",
    "    client_id=client_id,\n",
    "    secret_key=secret_key, \n",
    "    redirect_uri=redirect_uri, \n",
    "    response_type=response_type, \n",
    "    grant_type=grant_type\n",
    ")\n",
    "\n",
    "session.set_token(auth_code)\n",
    "\n",
    "response = session.generate_token()\n",
    "\n",
    "access_token = response['access_token']\n",
    "refresh_token = response['refresh_token']\n",
    "\n",
    "fyers = fyersModel.FyersModel(client_id=client_id, token=access_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty_50 = ['ITCHOTELS', 'INDIGO', 'MAXHEALTH', 'ADANIENT', 'ADANIPORTS', 'APOLLOHOSP', 'ASIANPAINT', 'AXISBANK', 'BAJAJFINSV', 'BAJAJ_AUTO', 'BAJFINANCE', 'BEL', 'BHARTIARTL', 'BPCL', 'BRITANNIA', 'CIPLA', 'COALINDIA', 'DIVISLAB', 'DRREDDY', 'EICHERMOT', 'ETERNAL', 'GAIL', 'GRASIM', 'HCLTECH', 'HDFCBANK', 'HDFCLIFE', 'HEROMOTOCO', 'HINDALCO', 'HINDUNILVR', 'ICICIBANK', 'INDUSINDBK', 'INFY', 'IOC', 'ITC', 'JIOFIN', 'JSWSTEEL', 'KOTAKBANK', 'LTIM', 'LT', 'M&M', 'MARUTI', 'NESTLEIND', 'NTPC', 'ONGC', 'POWERGRID', 'RELIANCE', 'SBILIFE', 'SBIN', 'SHREECEM', 'SHRIRAMFIN', 'SUNPHARMA', 'TATACONSUM', 'TATAMOTORS', 'TATASTEEL', 'TCS', 'TECHM', 'TITAN', 'TRENT', 'ULTRACEMCO', 'UPL', 'WIPRO', 'YESBANK']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------------- find stock in masterlist file -----------------------------------#\n",
    "\n",
    "# Read Master_list file of Fyers\n",
    "df = pd.read_csv(r\"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/fyers_master_list.csv\")\n",
    "df = df[df['NSE:GOLDSTAR-SM'].str.contains('-EQ', na=False)].reset_index(drop=True)\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "n = 0\n",
    "\n",
    "nifty_50 = [symbol.replace(\"_\", \"-\") for symbol in nifty_50]\n",
    "\n",
    "x = 0\n",
    "# Fix: Iterate over the actual values in the list, not indices\n",
    "for stock in nifty_50:\n",
    "    if f\"NSE:{stock}-EQ\" in df[\"NSE:GOLDSTAR-SM\"].values:\n",
    "        x += 1\n",
    "        n += 1\n",
    "        # a = print(x, stock, \"\\tFound\") if len(stock) > 4 else print(x, stock, \"\\t\\tFound\")\n",
    "    else:\n",
    "        x += 1\n",
    "        print(x, stock, \"\\tNot found\")\n",
    "\n",
    "print(f\"From {len(nifty_50)} Total {n} Nifty50 stocks found in the DataFrame.\")\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "nifty_50_tokens = [f\"NSE:{stock}-EQ\" for stock in nifty_50]\n",
    "filtered_masterlist = df[df['NSE:GOLDSTAR-SM'].isin(nifty_50_tokens)].reset_index(drop=True)\n",
    "nifty50_stockcode = filtered_masterlist['NSE:GOLDSTAR-SM'].to_list()\n",
    "print(nifty50_stockcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_stock_data(symbol, days, resolution, range_from, range_to):\n",
    "\n",
    "#     try:\n",
    "#         start_date = datetime.strptime(range_from, \"%Y-%m-%d\")\n",
    "#         end_date = datetime.strptime(range_to, \"%Y-%m-%d\")\n",
    "        \n",
    "#         all_data = []\n",
    "        \n",
    "#         chunks = []\n",
    "#         current_start = start_date\n",
    "#         while current_start < end_date:\n",
    "#             current_end = min(current_start + timedelta(days=days), end_date)\n",
    "#             chunks.append((current_start.strftime(\"%Y-%m-%d\"), current_end.strftime(\"%Y-%m-%d\")))\n",
    "#             current_start = current_end + timedelta(days=1)\n",
    "#         print(chunks)\n",
    "\n",
    "#         for chunk_from, chunk_to in chunks:\n",
    "#             data = fyers.history(data={\"symbol\": symbol, \n",
    "#                                        \"resolution\": resolution, \n",
    "#                                        \"date_format\": \"1\", \n",
    "#                                        \"range_from\": chunk_from, \n",
    "#                                        \"range_to\": chunk_to, \n",
    "#                                        \"cont_flag\": \"1\"})\n",
    "            \n",
    "#             # print(data)\n",
    "                        \n",
    "#             if data.get('s') == 'ok' and data.get('candles'):\n",
    "#                 all_data.extend(data['candles'])\n",
    "#             else:\n",
    "#                 print(f\"Error: {data}\")\n",
    "\n",
    "#             time.sleep(1)  # To avoid hitting rate limits\n",
    "\n",
    "#         data = pd.DataFrame(all_data, columns=['timestamp', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "#         data['Datetime'] = pd.to_datetime(data['timestamp'], unit='s', utc=True).dt.tz_convert('Asia/Kolkata')\n",
    "#         data['Datetime'] = data['Datetime'].dt.tz_localize(None)\n",
    "#         data = data[['Datetime', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "#         return data\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error downloading {symbol}: {str(e)}\")\n",
    "#         return None\n",
    "\n",
    "# data = download_stock_data('NSE:RELIANCE-EQ', \n",
    "#                            days=10, \n",
    "#                            resolution=\"5S\", \n",
    "#                            range_from=\"2025-01-01\", \n",
    "#                            range_to=\"2025-10-06\")\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- Helper Functions -----------------------------------\n",
    "\n",
    "def calculate_expected_candles(resolution, trading_days):\n",
    "    \"\"\"\n",
    "    Calculate expected number of candles for given resolution and trading days.\n",
    "    Market hours: 9:15 AM to 3:30 PM = 375 minutes = 22,500 seconds\n",
    "    \"\"\"\n",
    "    total_seconds = 22500  # 6h 15min in seconds\n",
    "\n",
    "    resolution_map = {\n",
    "        # Seconds based\n",
    "        \"5S\": 5, \"10S\": 10, \"15S\": 15, \"30S\": 30, \"45S\": 45,\n",
    "        # Minutes based\n",
    "        \"1\": 60, \"2\": 120, \"3\": 180, \"5\": 300, \"10\": 600, \"15\": 900,\n",
    "        \"30\": 1800, \"60\": 3600,\n",
    "        # Daily\n",
    "        \"D\": None\n",
    "    }\n",
    "\n",
    "    if resolution == \"D\":\n",
    "        return trading_days\n",
    "\n",
    "    if resolution not in resolution_map:\n",
    "        raise ValueError(f\"Unsupported resolution: {resolution}\")\n",
    "\n",
    "    candles_per_day = total_seconds // resolution_map[resolution]\n",
    "    return trading_days * candles_per_day\n",
    "\n",
    "\n",
    "def get_trading_days(start_date, end_date, reference_csv=\"D:/Programming/Download_Backtest_Deploy_data/1__Download/TradingView_data_download/NIFTY_50_historical_data.csv\"):\n",
    "    \"\"\"\n",
    "    Extract trading days from reference CSV between date range.\n",
    "    Returns: list of trading day dates (YYYY-MM-DD format)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(reference_csv)\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "        start = pd.to_datetime(start_date)\n",
    "        end = pd.to_datetime(end_date)\n",
    "\n",
    "        mask = (df['Datetime'].dt.date >= start.date()) & (df['Datetime'].dt.date <= end.date())\n",
    "        trading_days = df.loc[mask, 'Datetime'].dt.date.tolist()\n",
    "\n",
    "        return [str(day) for day in trading_days]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading trading days: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def verify_data_completeness(data, expected_candles, trading_days):\n",
    "    \"\"\"\n",
    "    Verify downloaded data completeness and identify missing days.\n",
    "    Returns: dict with verification results\n",
    "    \"\"\"\n",
    "    if data is None or data.empty:\n",
    "        return {\n",
    "            \"status\": \"failed\",\n",
    "            \"actual_candles\": 0,\n",
    "            \"expected_candles\": expected_candles,\n",
    "            \"missing_days\": trading_days,\n",
    "            \"completeness_pct\": 0.0\n",
    "        }\n",
    "\n",
    "    actual_candles = len(data)\n",
    "    data['Date'] = pd.to_datetime(data['Datetime']).dt.date.astype(str)\n",
    "    available_days = data['Date'].unique().tolist()\n",
    "    missing_days = [day for day in trading_days if day not in available_days]\n",
    "\n",
    "    completeness_pct = (actual_candles / expected_candles * 100) if expected_candles > 0 else 0\n",
    "\n",
    "    status = \"complete\" if completeness_pct >= 99.5 else \"incomplete\"\n",
    "\n",
    "    return {\n",
    "        \"status\": status,\n",
    "        \"actual_candles\": actual_candles,\n",
    "        \"expected_candles\": expected_candles,\n",
    "        \"missing_days\": missing_days,\n",
    "        \"completeness_pct\": round(completeness_pct, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "def load_data_status():\n",
    "    \"\"\"Load status tracking JSON for all stocks\"\"\"\n",
    "    status_file = \"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/fyers_data_status.json\"\n",
    "    try:\n",
    "        if os.path.exists(status_file):\n",
    "            with open(status_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "    except:\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "\n",
    "def save_data_status(status_data):\n",
    "    \"\"\"Save status tracking JSON for all stocks\"\"\"\n",
    "    status_file = \"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/fyers_data_status.json\"\n",
    "    with open(status_file, 'w') as f:\n",
    "        json.dump(status_data, f, indent=2)\n",
    "\n",
    "\n",
    "def download_stock_data(symbol, days, resolution, range_from, range_to):\n",
    "    \"\"\"Download stock data from Fyers API\"\"\"\n",
    "    try:\n",
    "        start_date = datetime.strptime(range_from, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(range_to, \"%Y-%m-%d\")\n",
    "        \n",
    "        all_data = []\n",
    "        \n",
    "        chunks = []\n",
    "        current_start = start_date\n",
    "        while current_start < end_date:\n",
    "            current_end = min(current_start + timedelta(days=days), end_date)\n",
    "            chunks.append((current_start.strftime(\"%Y-%m-%d\"), current_end.strftime(\"%Y-%m-%d\")))\n",
    "            current_start = current_end + timedelta(days=1)\n",
    "\n",
    "        for chunk_from, chunk_to in chunks:\n",
    "            data = fyers.history(data={\"symbol\": symbol, \n",
    "                                       \"resolution\": resolution, \n",
    "                                       \"date_format\": \"1\", \n",
    "                                       \"range_from\": chunk_from, \n",
    "                                       \"range_to\": chunk_to, \n",
    "                                       \"cont_flag\": \"1\"})\n",
    "            \n",
    "            if data.get('s') == 'ok' and data.get('candles'):\n",
    "                all_data.extend(data['candles'])\n",
    "            else:\n",
    "                print(f\"Error: {data}\")\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "        data = pd.DataFrame(all_data, columns=['timestamp', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "        data['Datetime'] = pd.to_datetime(data['timestamp'], unit='s', utc=True).dt.tz_convert('Asia/Kolkata')\n",
    "        data['Datetime'] = data['Datetime'].dt.tz_localize(None)\n",
    "        data = data[['Datetime', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {symbol}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def download_with_verification(symbol, resolution, range_from, range_to, days=10, retries=3):\n",
    "    \"\"\"\n",
    "    Download data with verification and retry logic.\n",
    "    Returns: (data_df, verification_result, best_attempt)\n",
    "    \"\"\"\n",
    "    trading_days = get_trading_days(range_from, range_to)\n",
    "    expected_candles = calculate_expected_candles(resolution, len(trading_days))\n",
    "\n",
    "    best_data = None\n",
    "    best_candles = 0\n",
    "    best_verification = None\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        print(f\"\\nAttempt {attempt + 1}/{retries} for {symbol} ({resolution})\")\n",
    "\n",
    "        data = download_stock_data(symbol, days, resolution, range_from, range_to)\n",
    "        verification = verify_data_completeness(data, expected_candles, trading_days)\n",
    "\n",
    "        print(f\"Downloaded: {verification['actual_candles']}/{verification['expected_candles']} candles ({verification['completeness_pct']}%)\")\n",
    "\n",
    "        # Track best attempt\n",
    "        if verification['actual_candles'] > best_candles:\n",
    "            best_data = data\n",
    "            best_candles = verification['actual_candles']\n",
    "            best_verification = verification\n",
    "\n",
    "        # If complete, return immediately\n",
    "        if verification['status'] == 'complete':\n",
    "            return data, verification, {\"candles\": best_candles, \"attempt\": attempt + 1}\n",
    "\n",
    "        if attempt < retries - 1:\n",
    "            print(f\"Retrying... ({len(verification['missing_days'])} missing days)\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    print(f\"\\nBest result after {retries} attempts: {best_candles}/{expected_candles} candles\")\n",
    "    return best_data, best_verification, {\"candles\": best_candles, \"attempt\": retries}\n",
    "\n",
    "\n",
    "def fill_missing_data(symbol, resolution, missing_days, existing_data, days=1):\n",
    "    \"\"\"\n",
    "    Download data for specific missing days and merge with existing data.\n",
    "    Returns: merged DataFrame\n",
    "    \"\"\"\n",
    "    if not missing_days:\n",
    "        return existing_data\n",
    "\n",
    "    print(f\"\\nFilling {len(missing_days)} missing days for {symbol}...\")\n",
    "\n",
    "    all_new_data = []\n",
    "\n",
    "    for day in missing_days:\n",
    "        print(f\"Downloading: {day}\")\n",
    "        data = download_stock_data(symbol, days, resolution, day, day)\n",
    "        if data is not None and not data.empty:\n",
    "            all_new_data.append(data)\n",
    "        time.sleep(1)\n",
    "\n",
    "    if all_new_data:\n",
    "        new_data = pd.concat(all_new_data, ignore_index=True)\n",
    "        merged_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "        merged_data = merged_data.drop_duplicates(subset=['Datetime']).sort_values('Datetime').reset_index(drop=True)\n",
    "        print(f\"Added {len(new_data)} candles. Total: {len(merged_data)}\")\n",
    "        return merged_data\n",
    "\n",
    "    return existing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def master_download(resolution=\"5S\", range_from=\"2025-01-01\", range_to=\"2025-10-06\"):\n",
    "    \"\"\"\n",
    "    Master download function with verification, retry, and interactive handling.\n",
    "    Similar to TV1.ipynb mechanism with automated missing data filling.\n",
    "    Data stored in: D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data folder path\n",
    "    data_folder = \"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API\"\n",
    "    \n",
    "    # Load existing status\n",
    "    status_data = load_data_status()\n",
    "    \n",
    "    successful = []\n",
    "    failed = []\n",
    "    incomplete = []\n",
    "    \n",
    "    print(f\"=== Starting Master Download ===\")\n",
    "    print(f\"Resolution: {resolution}\")\n",
    "    print(f\"Date Range: {range_from} to {range_to}\")\n",
    "    print(f\"Total Stocks: {len(nifty50_stockcode)}\\n\")\n",
    "    \n",
    "    for i, symbol in enumerate(nifty50_stockcode, 1):\n",
    "        stock_name = symbol.replace(\"NSE:\", \"\").replace(\"-EQ\", \"\")\n",
    "        filename = f\"{data_folder}/{stock_name}_{resolution}.csv\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[{i}/{len(nifty50_stockcode)}] Processing: {stock_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Initialize stock status if not exists\n",
    "        if stock_name not in status_data:\n",
    "            status_data[stock_name] = {}\n",
    "        \n",
    "        if resolution not in status_data[stock_name]:\n",
    "            status_data[stock_name][resolution] = {}\n",
    "        \n",
    "        # Check if file already exists\n",
    "        if os.path.exists(filename):\n",
    "            existing_data = pd.read_csv(filename)\n",
    "            print(f\"✓ Existing file found: {len(existing_data)} candles\")\n",
    "            \n",
    "            # Verify existing data\n",
    "            trading_days = get_trading_days(range_from, range_to)\n",
    "            expected_candles = calculate_expected_candles(resolution, len(trading_days))\n",
    "            verification = verify_data_completeness(existing_data, expected_candles, trading_days)\n",
    "            \n",
    "            print(f\"Status: {verification['completeness_pct']}% complete ({verification['actual_candles']}/{verification['expected_candles']})\")\n",
    "            \n",
    "            if verification['status'] == 'complete':\n",
    "                print(\"✓ Data is complete. Skipping.\")\n",
    "                successful.append(stock_name)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"⚠ Missing {len(verification['missing_days'])} days. Will attempt to fill...\")\n",
    "                # Continue to download/fill logic below\n",
    "        else:\n",
    "            existing_data = None\n",
    "            verification = None\n",
    "        \n",
    "        # Download with verification\n",
    "        data, verification, best_attempt = download_with_verification(\n",
    "            symbol, resolution, range_from, range_to, days=10, retries=3\n",
    "        )\n",
    "        \n",
    "        # Update status\n",
    "        status_data[stock_name][resolution] = {\n",
    "            \"last_updated\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"date_range\": {\"from\": range_from, \"to\": range_to},\n",
    "            \"trading_days_expected\": len(get_trading_days(range_from, range_to)),\n",
    "            \"expected_candles\": verification['expected_candles'],\n",
    "            \"actual_candles\": verification['actual_candles'],\n",
    "            \"missing_days\": verification['missing_days'],\n",
    "            \"status\": verification['status'],\n",
    "            \"completeness_pct\": verification['completeness_pct'],\n",
    "            \"retry_count\": best_attempt['attempt'],\n",
    "            \"best_attempt\": best_attempt\n",
    "        }\n",
    "        \n",
    "        # Handle based on status\n",
    "        if verification['status'] == 'complete':\n",
    "            data.to_csv(filename, index=False)\n",
    "            successful.append(stock_name)\n",
    "            save_data_status(status_data)\n",
    "            print(f\"✓ Success: {stock_name}\")\n",
    "            \n",
    "        elif verification['status'] == 'incomplete' and len(verification['missing_days']) > 0:\n",
    "            # Try to fill missing days\n",
    "            print(f\"\\n⚠ Incomplete download. Attempting to fill {len(verification['missing_days'])} missing days...\")\n",
    "            \n",
    "            filled_data = fill_missing_data(symbol, resolution, verification['missing_days'], data, days=1)\n",
    "            \n",
    "            # Re-verify after filling\n",
    "            trading_days = get_trading_days(range_from, range_to)\n",
    "            expected_candles = calculate_expected_candles(resolution, len(trading_days))\n",
    "            final_verification = verify_data_completeness(filled_data, expected_candles, trading_days)\n",
    "            \n",
    "            filled_data.to_csv(filename, index=False)\n",
    "            \n",
    "            # Update status with final results\n",
    "            status_data[stock_name][resolution].update({\n",
    "                \"actual_candles\": final_verification['actual_candles'],\n",
    "                \"missing_days\": final_verification['missing_days'],\n",
    "                \"status\": final_verification['status'],\n",
    "                \"completeness_pct\": final_verification['completeness_pct']\n",
    "            })\n",
    "            \n",
    "            if final_verification['status'] == 'complete':\n",
    "                successful.append(stock_name)\n",
    "                print(f\"✓ Successfully filled all missing data!\")\n",
    "            else:\n",
    "                incomplete.append(stock_name)\n",
    "                print(f\"⚠ Still incomplete: {final_verification['completeness_pct']}%\")\n",
    "                \n",
    "                # Ask user for action\n",
    "                user_choice = input(f\"\\nOptions:\\n1. Use best data ({final_verification['completeness_pct']}%)\\n2. Skip this stock\\n3. Retry again\\nChoice (1/2/3): \")\n",
    "                \n",
    "                if user_choice == '1':\n",
    "                    print(f\"✓ Using best available data: {final_verification['actual_candles']} candles\")\n",
    "                    status_data[stock_name][resolution]['user_action'] = 'used_best_data'\n",
    "                elif user_choice == '2':\n",
    "                    os.remove(filename) if os.path.exists(filename) else None\n",
    "                    failed.append(stock_name)\n",
    "                    print(f\"✗ Skipped: {stock_name}\")\n",
    "                elif user_choice == '3':\n",
    "                    # Manual retry - call recursively for this symbol\n",
    "                    print(\"Retrying...\")\n",
    "                    continue\n",
    "            \n",
    "            save_data_status(status_data)\n",
    "            \n",
    "        else:\n",
    "            failed.append(stock_name)\n",
    "            print(f\"✗ Failed: {stock_name}\")\n",
    "            save_data_status(status_data)\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Final Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"=== Download Summary ===\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✓ Successful: {len(successful)}/{len(nifty50_stockcode)}\")\n",
    "    print(f\"⚠ Incomplete: {len(incomplete)}/{len(nifty50_stockcode)}\")\n",
    "    print(f\"✗ Failed: {len(failed)}/{len(nifty50_stockcode)}\")\n",
    "    if failed:\n",
    "        print(f\"\\nFailed stocks: {failed}\")\n",
    "    if incomplete:\n",
    "        print(f\"\\nIncomplete stocks: {incomplete}\")\n",
    "    \n",
    "    return successful, incomplete, failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- Test Example -----------------------------------\n",
    "\n",
    "# Test with single stock first\n",
    "# data, verification, best = download_with_verification(\n",
    "#     'NSE:RELIANCE-EQ', \n",
    "#     resolution=\"5S\", \n",
    "#     range_from=\"2025-01-01\", \n",
    "#     range_to=\"2025-01-31\",\n",
    "#     days=10,\n",
    "#     retries=3\n",
    "# )\n",
    "\n",
    "# print(\"\\n=== Verification Results ===\")\n",
    "# print(f\"Status: {verification['status']}\")\n",
    "# print(f\"Candles: {verification['actual_candles']}/{verification['expected_candles']}\")\n",
    "# print(f\"Completeness: {verification['completeness_pct']}%\")\n",
    "# print(f\"Missing days: {len(verification['missing_days'])}\")\n",
    "\n",
    "# # Test master download with all stocks\n",
    "# successful, incomplete, failed = master_download(\n",
    "#     resolution=\"5S\",\n",
    "#     range_from=\"2025-01-01\", \n",
    "#     range_to=\"2025-01-31\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
