{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------- Download Fyers Master List ----------------- #\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Download the CSV file\n",
    "# url = \"https://public.fyers.in/sym_details/NSE_CM.csv\"\n",
    "# response = requests.get(url)\n",
    "\n",
    "# # Create directory if it doesn't exist\n",
    "# data_dir = \"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API\"\n",
    "# os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# # Save the file\n",
    "# file_path = os.path.join(data_dir, \"fyers_master_list.csv\")\n",
    "# with open(file_path, 'wb') as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# print(f\"Master list downloaded and saved to: {file_path}\")\n",
    "\n",
    "# # Load and display basic info\n",
    "# master_df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api-t1.fyers.in/api/v3/generate-authcode?client_id=UZJ1B1WF4N-100&redirect_uri=https%3A%2F%2Fwww.google.com%2F&response_type=code&state=None'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from fyers_apiv3 import fyersModel\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "\n",
    "client_id = \"UZJ1B1WF4N-100\"\n",
    "secret_key = \"KJ7CW6C2I8\"\n",
    "redirect_uri = \"https://www.google.com/\"\n",
    "response_type = \"code\"  \n",
    "state = \"sample_state\"\n",
    "grant_type = \"authorization_code\"  \n",
    "totp_secret = \"SFJM4CZGYCQZFK75K3AAMCKBPHNMABQ4\"\n",
    "pin = \"5555\"\n",
    "\n",
    "session = fyersModel.SessionModel(\n",
    "    client_id=client_id,\n",
    "    secret_key=secret_key,\n",
    "    redirect_uri=redirect_uri,\n",
    "    response_type=response_type\n",
    ")\n",
    "\n",
    "response = session.generate_authcode()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_code = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhcHBfaWQiOiJVWkoxQjFXRjROIiwidXVpZCI6IjA4NzhiYzM4MjQwOTRkZjM5YzU1ZGI2NGVkNmM0ZGQyIiwiaXBBZGRyIjoiIiwibm9uY2UiOiIiLCJzY29wZSI6IiIsImRpc3BsYXlfbmFtZSI6IllLMDg5NzMiLCJvbXMiOiJLMSIsImhzbV9rZXkiOiI5YjRmM2VmMDZmYWJhNjIwN2E4MWQ4Y2RkNjUxZjVkMWI2ZDhmMDdkMjUyYjc4YjA0ZDYzYTYyNyIsImlzRGRwaUVuYWJsZWQiOiJZIiwiaXNNdGZFbmFibGVkIjoiTiIsImF1ZCI6IltcImQ6MVwiLFwiZDoyXCIsXCJ4OjBcIixcIng6MVwiLFwieDoyXCJdIiwiZXhwIjoxNzYwMDIyMTg1LCJpYXQiOjE3NTk5OTIxODUsImlzcyI6ImFwaS5sb2dpbi5meWVycy5pbiIsIm5iZiI6MTc1OTk5MjE4NSwic3ViIjoiYXV0aF9jb2RlIn0.OpT5SiUlziwrC0Ys3SVDO5voPh_Hd4zGXiQjovGHBUo\"\n",
    "\n",
    "session = fyersModel.SessionModel(\n",
    "    client_id=client_id,\n",
    "    secret_key=secret_key, \n",
    "    redirect_uri=redirect_uri, \n",
    "    response_type=response_type, \n",
    "    grant_type=grant_type\n",
    ")\n",
    "\n",
    "session.set_token(auth_code)\n",
    "\n",
    "response = session.generate_token()\n",
    "\n",
    "access_token = response['access_token']\n",
    "refresh_token = response['refresh_token']\n",
    "\n",
    "fyers = fyersModel.FyersModel(client_id=client_id, token=access_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nifty_50_componants_data_from_tradingview.csv\n",
    "nifty_comprehensive_df = pd.read_csv(\"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_ICICI_via_API/nifty_50_componants_data_from_tradingview.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Master_list file of Fyers\n",
    "df = pd.read_csv(r\"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/fyers_master_list.csv\")\n",
    "df = df[df['NSE:GOLDSTAR-SM'].str.contains('-EQ', na=False)].reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------------- find stock in masterlist file -----------------------------------#\n",
    "\n",
    "n = 0\n",
    "\n",
    "# Nifty 50 symbols to find\n",
    "nifty_50 = nifty_comprehensive_df['Symbol'].values.tolist()\n",
    "nifty_50 = [symbol.replace(\"_\", \"-\") for symbol in nifty_50]\n",
    "\n",
    "x = 0\n",
    "# Fix: Iterate over the actual values in the list, not indices\n",
    "for stock in nifty_50:\n",
    "    if f\"NSE:{stock}-EQ\" in df[\"NSE:GOLDSTAR-SM\"].values:\n",
    "        x += 1\n",
    "        n += 1\n",
    "        # a = print(x, stock, \"\\tFound\") if len(stock) > 4 else print(x, stock, \"\\t\\tFound\")\n",
    "    else:\n",
    "        x += 1\n",
    "        print(x, stock, \"\\tNot found\")\n",
    "\n",
    "print(f\"Total {n} Nifty 50 stocks found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty_50_tokens = [f\"NSE:{stock}-EQ\" for stock in nifty_50]\n",
    "filtered_masterlist = df[df['NSE:GOLDSTAR-SM'].isin(nifty_50_tokens)].reset_index(drop=True)\n",
    "nifty50_stockcode = filtered_masterlist['NSE:GOLDSTAR-SM'].to_list()\n",
    "print(nifty50_stockcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Nifty 50 components data and save to folder\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create directory for saving data\n",
    "save_dir = \"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/nifty50_daily_data\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Date range for download\n",
    "date_range = {\n",
    "    \"resolution\": \"D\",\n",
    "    \"date_format\": \"1\", \n",
    "    \"range_from\": \"2025-05-01\", \n",
    "    \"range_to\": \"2025-09-23\", \n",
    "    \"cont_flag\": \"1\"\n",
    "}\n",
    "\n",
    "print(f\"Starting download of {len(nifty50_stockcode)} Nifty 50 stocks...\")\n",
    "print(f\"Date range: {date_range['range_from']} to {date_range['range_to']}\")\n",
    "print(f\"Saving to: {save_dir}\")\n",
    "\n",
    "successful_downloads = 0\n",
    "failed_downloads = []\n",
    "\n",
    "for i, symbol in enumerate(nifty50_stockcode, 1):\n",
    "    try:\n",
    "        # Prepare data for API call\n",
    "        data = {\n",
    "            \"symbol\": symbol,\n",
    "            **date_range\n",
    "        }\n",
    "        \n",
    "        # Make API call\n",
    "        response = fyers.history(data=data)\n",
    "        \n",
    "        if response.get('s') == 'ok' and response.get('candles'):\n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(response['candles'], columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "            df['datetime'] = pd.to_datetime(df['timestamp'], unit='s', utc=True).dt.tz_convert('Asia/Kolkata')\n",
    "            df['symbol'] = symbol.replace(\"NSE:\", \"\").replace(\"-EQ\", \"\")\n",
    "            df = df[['symbol', 'datetime', 'open', 'high', 'low', 'close', 'volume']]\n",
    "            df['datetime'] = df['datetime'].dt.tz_localize(None)\n",
    "            \n",
    "            # Save to CSV\n",
    "            filename = f\"{symbol.replace('NSE:', '').replace('-EQ', '')}_daily_data.csv\"\n",
    "            filepath = os.path.join(save_dir, filename)\n",
    "            df.to_csv(filepath, index=False)\n",
    "            \n",
    "            successful_downloads += 1\n",
    "            print(f\"{i:2d}. {symbol:20s} - Downloaded {len(df)} records -> {filename}\")\n",
    "            \n",
    "        else:\n",
    "            failed_downloads.append(symbol)\n",
    "            print(f\"{i:2d}. {symbol:20s} - Failed: {response.get('message', 'Unknown error')}\")\n",
    "        \n",
    "        # Add small delay to avoid rate limiting\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed_downloads.append(symbol)\n",
    "        print(f\"{i:2d}. {symbol:20s} - Error: {str(e)}\")\n",
    "\n",
    "print(f\"\\nDownload Summary:\")\n",
    "print(f\"Successful: {successful_downloads}\")\n",
    "print(f\"Failed: {len(failed_downloads)}\")\n",
    "if failed_downloads:\n",
    "    print(f\"Failed symbols: {failed_downloads}\")\n",
    "print(f\"Data saved in: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum all stocks volume by datetime\n",
    "import glob\n",
    "\n",
    "# Path to the folder containing individual stock CSV files\n",
    "data_folder = \"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/nifty50_daily_data\"\n",
    "\n",
    "# Get all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(data_folder, \"*_daily_data.csv\"))\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files to process...\")\n",
    "\n",
    "# Initialize list to store all dataframes\n",
    "all_dataframes = []\n",
    "\n",
    "# Read each CSV file and collect dataframes\n",
    "for file_path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        # Keep only datetime and volume columns with symbol info\n",
    "        df_volume = df[['datetime', 'volume', 'symbol']].copy()\n",
    "        all_dataframes.append(df_volume)\n",
    "        # print(f\"Loaded: {os.path.basename(file_path)} - {len(df)} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {os.path.basename(file_path)}: {str(e)}\")\n",
    "\n",
    "if all_dataframes:\n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Group by datetime and sum volumes\n",
    "    volume_summary = combined_df.groupby('datetime').agg({\n",
    "        'volume': 'sum',\n",
    "        'symbol': 'count'  # Count how many stocks contributed to each date\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    volume_summary.rename(columns={'symbol': 'stock_count'}, inplace=True)\n",
    "    \n",
    "    # Sort by datetime\n",
    "    volume_summary = volume_summary.sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nVolume Summary (Total across all Nifty 50 stocks):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(volume_summary.to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"No data files found to process!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_summary['volume'] = round((volume_summary['volume'] / 1000000), 2)\n",
    "volume_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------- Testing cell ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tvml_df with both TradingView and Fyers symbol formats\n",
    "# Read TradingView Nifty 50 components file\n",
    "tv_df = pd.read_csv('D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_ICICI_via_API/nifty_50_componants_data_from_tradingview.csv')\n",
    "\n",
    "# Extract symbols and company names\n",
    "symbols = tv_df['Symbol'].tolist()\n",
    "company_names = tv_df['Company_Name'].tolist()\n",
    "\n",
    "# Handle special case for BAJAJ_AUTO\n",
    "symbols = [symbol.replace(\"_\", \"-\") for symbol in symbols]\n",
    "\n",
    "# Create tvml_df with both formats\n",
    "tvml_df = pd.DataFrame({\n",
    "    'tv_symbol': symbols,\n",
    "    'company_names': company_names\n",
    "})\n",
    "\n",
    "tvml_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create n5v_df with symbol and volume for date 2025-08-11\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the folder containing individual stock CSV files\n",
    "data_folder = \"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/nifty50_daily_data\"\n",
    "\n",
    "# Get all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(data_folder, \"*_daily_data.csv\"))\n",
    "\n",
    "# Target date\n",
    "target_date = '2025-08-22'\n",
    "\n",
    "# Initialize list to store data for target date\n",
    "data_for_date = []\n",
    "\n",
    "# Read each CSV file and extract data for target date\n",
    "for file_path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        \n",
    "        # Filter for target date\n",
    "        target_data = df[df['datetime'].dt.date == pd.to_datetime(target_date).date()]\n",
    "        \n",
    "        if not target_data.empty:\n",
    "            symbol = target_data['symbol'].iloc[0]\n",
    "            volume = target_data['volume'].iloc[0]\n",
    "            data_for_date.append({'symbol': symbol, 'volume': volume})\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {os.path.basename(file_path)}: {str(e)}\")\n",
    "\n",
    "# Create n5v_df\n",
    "n5v_df = pd.DataFrame(data_for_date)\n",
    "\n",
    "# Sort by symbol for better readability\n",
    "n5v_df = n5v_df.sort_values('symbol').reset_index(drop=True)\n",
    "\n",
    "print(f\"n5v_df - Nifty 50 Volume Data for {target_date}\")\n",
    "print(\"=\" * 50)\n",
    "print(n5v_df.to_string(index=False))\n",
    "print(f\"\\nTotal stocks: {len(n5v_df)}\")\n",
    "print(f\"Total volume: {n5v_df['volume'].sum()/1000000:,}\")\n",
    "\n",
    "ea = n5v_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First verify symbol matching between ea and tvml_df\n",
    "print(\"Symbol Matching Verification:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get unique symbols from both DataFrames\n",
    "ea_symbols = set(ea['symbol'].tolist())\n",
    "tvml_symbols = set(tvml_df['tv_symbol'].tolist())\n",
    "\n",
    "# Find matches and mismatches\n",
    "matched_symbols = ea_symbols.intersection(tvml_symbols)\n",
    "ea_only = ea_symbols - tvml_symbols\n",
    "tvml_only = tvml_symbols - ea_symbols\n",
    "\n",
    "print(f\"EA symbols: {len(ea_symbols)}\")\n",
    "print(f\"TVML symbols: {len(tvml_symbols)}\")\n",
    "print(f\"Matched symbols: {len(matched_symbols)}\")\n",
    "print(f\"EA only symbols: {len(ea_only)}\")\n",
    "print(f\"TVML only symbols: {len(tvml_only)}\")\n",
    "\n",
    "if ea_only:\n",
    "    print(f\"\\nSymbols in EA but not in TVML: {sorted(ea_only)}\")\n",
    "if tvml_only:\n",
    "    print(f\"\\nSymbols in TVML but not in EA: {sorted(tvml_only)}\")\n",
    "\n",
    "print(f\"\\nMatched symbols: {sorted(matched_symbols)}\")\n",
    "\n",
    "# After verification, add company names to ea DataFrame\n",
    "# Note: Using 'company_names' (plural) as per tvml_df column name\n",
    "ea_with_names = ea.merge(tvml_df[['tv_symbol', 'company_names']], \n",
    "                        left_on='symbol', \n",
    "                        right_on='tv_symbol', \n",
    "                        how='left')\n",
    "\n",
    "# Drop the duplicate tv_symbol column and reorder columns\n",
    "ea_with_names = ea_with_names[['symbol', 'company_names', 'volume']].copy()\n",
    "\n",
    "# Rename column to singular for consistency\n",
    "ea_with_names.rename(columns={'company_names': 'company_name'}, inplace=True)\n",
    "\n",
    "print(f\"\\nEA DataFrame with Company Names:\")\n",
    "print(\"=\" * 50)\n",
    "ea_with_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ea_with_names in D:\\Programming\\Download_Backtest_Deploy\\1__Download\\1__Download_data_Fyers_via_API\n",
    "ea_with_names.to_csv('D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/ea_with_names.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________ Analysis code ___________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load both ea_with_names DataFrames\n",
    "fyers_df = pd.read_csv('D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/ea_with_names.csv')\n",
    "icici_df = pd.read_csv('D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_ICICI_via_API/ea_with_names.csv')\n",
    "\n",
    "# Merge on company_name to align the data\n",
    "merged_df = pd.merge(fyers_df, icici_df, on='company_name', suffixes=('_fyers', '_icici'), how='outer')\n",
    "\n",
    "# Calculate volume differences\n",
    "merged_df['volume_diff'] = merged_df['volume_fyers'] - merged_df['volume_icici']\n",
    "merged_df['volume_diff_pct'] = (merged_df['volume_diff'] / merged_df['volume_icici']) * 100\n",
    "\n",
    "# Total volumes\n",
    "total_fyers = merged_df['volume_fyers'].sum()\n",
    "total_icici = merged_df['volume_icici'].sum()\n",
    "total_diff = total_fyers - total_icici\n",
    "total_diff_pct = (total_diff / total_icici) * 100\n",
    "merged_df['pct'] = round((((merged_df['volume_fyers'] - merged_df['volume_icici'])) / merged_df['volume_icici']) * 100, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['volume_fyers'] = round((merged_df['volume_fyers'] / 1000000), 2)\n",
    "merged_df['volume_icici'] = round((merged_df['volume_icici'] / 1000000), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
