{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from fyers_apiv3 import fyersModel\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import time\n",
    "\n",
    "CONFIG_FILE = \"fyers_config.json\"\n",
    "\n",
    "def get_fyers():\n",
    "    \"\"\"Get Fyers client with auto token management\"\"\"\n",
    "    config = json.load(open(CONFIG_FILE))\n",
    "    \n",
    "    # Try existing tokens\n",
    "    if 'access_token' in config:\n",
    "        fyers = fyersModel.FyersModel(client_id=config['client_id'], token=config['access_token'])\n",
    "        try:\n",
    "            if fyers.get_profile().get('s') == 'ok':\n",
    "                return fyers\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Try refresh\n",
    "        if 'refresh_token' in config:\n",
    "            try:\n",
    "                session = fyersModel.SessionModel(\n",
    "                    client_id=config['client_id'],\n",
    "                    secret_key=config['secret_key'],\n",
    "                    redirect_uri=config['redirect_uri'],\n",
    "                    response_type=\"code\",\n",
    "                    grant_type=\"refresh_token\"\n",
    "                )\n",
    "                session.set_token(config['refresh_token'])\n",
    "                resp = session.generate_token()\n",
    "                if resp.get('s') == 'ok':\n",
    "                    config['access_token'] = resp['access_token']\n",
    "                    config['refresh_token'] = resp['refresh_token']\n",
    "                    json.dump(config, open(CONFIG_FILE, 'w'), indent=4)\n",
    "                    return fyersModel.FyersModel(client_id=config['client_id'], token=resp['access_token'])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # New auth\n",
    "    session = fyersModel.SessionModel(\n",
    "        client_id=config['client_id'],\n",
    "        secret_key=config['secret_key'],\n",
    "        redirect_uri=config['redirect_uri'],\n",
    "        response_type=\"code\"\n",
    "    )\n",
    "    print(f\"Visit: {session.generate_authcode()}\")\n",
    "    auth_code = input(\"Enter auth_code: \").strip()\n",
    "    \n",
    "    session.grant_type = \"authorization_code\"\n",
    "    session.set_token(auth_code)\n",
    "    resp = session.generate_token()\n",
    "    \n",
    "    config['access_token'] = resp['access_token']\n",
    "    config['refresh_token'] = resp['refresh_token']\n",
    "    json.dump(config, open(CONFIG_FILE, 'w'), indent=4)\n",
    "    return fyersModel.FyersModel(client_id=config['client_id'], token=resp['access_token'])\n",
    "\n",
    "fyers = get_fyers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty_50_raw = ['ITCHOTELS', 'INDIGO', 'MAXHEALTH', 'ADANIENT', 'ADANIPORTS', 'APOLLOHOSP', 'ASIANPAINT', 'AXISBANK', 'BAJAJFINSV', 'BAJAJ_AUTO', 'BAJFINANCE', 'BEL', 'BHARTIARTL', 'BPCL', 'BRITANNIA', 'CIPLA', 'COALINDIA', 'DIVISLAB', 'DRREDDY', 'EICHERMOT', 'ETERNAL', 'GAIL', 'GRASIM', 'HCLTECH', 'HDFCBANK', 'HDFCLIFE', 'HEROMOTOCO', 'HINDALCO', 'HINDUNILVR', 'ICICIBANK', 'INDUSINDBK', 'INFY', 'IOC', 'ITC', 'JIOFIN', 'JSWSTEEL', 'KOTAKBANK', 'LTIM', 'LT', 'M&M', 'MARUTI', 'NESTLEIND', 'NTPC', 'ONGC', 'POWERGRID', 'RELIANCE', 'SBILIFE', 'SBIN', 'SHREECEM', 'SHRIRAMFIN', 'SUNPHARMA', 'TATACONSUM', 'TATAMOTORS', 'TATASTEEL', 'TCS', 'TECHM', 'TITAN', 'TRENT', 'ULTRACEMCO', 'UPL', 'WIPRO', 'YESBANK']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------------- find stock in masterlist file -----------------------------------#\n",
    "\n",
    "# Read Master_list file of Fyers\n",
    "masterlist_fyers_df = pd.read_csv(r\"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/fyers_master_list.csv\")\n",
    "masterlist_fyers_df = masterlist_fyers_df[masterlist_fyers_df['NSE:GOLDSTAR-SM'].str.contains('-EQ', na=False)].reset_index(drop=True)\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "n = 0\n",
    "\n",
    "nifty_50_raw = [symbol.replace(\"_\", \"-\") for symbol in nifty_50_raw]\n",
    "\n",
    "x = 0\n",
    "# Fix: Iterate over the actual values in the list, not indices\n",
    "for stock in nifty_50_raw:\n",
    "    if f\"NSE:{stock}-EQ\" in masterlist_fyers_df[\"NSE:GOLDSTAR-SM\"].values:\n",
    "        x += 1\n",
    "        n += 1\n",
    "        # a = print(x, stock, \"\\tFound\") if len(stock) > 4 else print(x, stock, \"\\t\\tFound\")\n",
    "    else:\n",
    "        x += 1\n",
    "        print(x, stock, \"\\tNot found\")\n",
    "\n",
    "print(f\"From {len(nifty_50_raw)} Total {n} Nifty50 stocks found in the DataFrame.\")\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "nifty_50_tokens = [f\"NSE:{stock}-EQ\" for stock in nifty_50_raw]\n",
    "filtered_masterlist = masterlist_fyers_df[masterlist_fyers_df['NSE:GOLDSTAR-SM'].isin(nifty_50_tokens)].reset_index(drop=True)\n",
    "nifty_50 = filtered_masterlist['NSE:GOLDSTAR-SM'].to_list()\n",
    "\n",
    "# # Comment this after downloading Nifty 50 Index data\n",
    "# nifty_50.append(\"NSE:NIFTY50-INDEX\") \n",
    "\n",
    "print(nifty_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stock_data(symbol, days, resolution, range_from, range_to):\n",
    "\n",
    "    try:\n",
    "        start_date = datetime.strptime(range_from, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(range_to, \"%Y-%m-%d\")\n",
    "        \n",
    "        all_data = []\n",
    "        \n",
    "        chunks = []\n",
    "        current_start = start_date\n",
    "        while current_start <= end_date:\n",
    "            current_end = min(current_start + timedelta(days=days), end_date)\n",
    "            chunks.append((current_start.strftime(\"%Y-%m-%d\"), current_end.strftime(\"%Y-%m-%d\")))\n",
    "            current_start = current_end + timedelta(days=1)\n",
    "        # print(chunks)\n",
    "\n",
    "        for chunk_from, chunk_to in chunks:\n",
    "            data = fyers.history(data={\"symbol\": symbol, \n",
    "                                       \"resolution\": resolution, \n",
    "                                       \"date_format\": \"1\", \n",
    "                                       \"range_from\": chunk_from, \n",
    "                                       \"range_to\": chunk_to, \n",
    "                                       \"cont_flag\": \"1\"})\n",
    "\n",
    "            # print(data)\n",
    "\n",
    "            if data.get('s') == 'ok' and data.get('candles'):\n",
    "                all_data.extend(data['candles'])\n",
    "            else:\n",
    "                print(f\"Error: {data}\")\n",
    "\n",
    "            time.sleep(1)  # To avoid hitting rate limits\n",
    "\n",
    "        data = pd.DataFrame(all_data, columns=['timestamp', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "        data['Datetime'] = pd.to_datetime(data['timestamp'], unit='s', utc=True).dt.tz_convert('Asia/Kolkata')\n",
    "        data['Datetime'] = data['Datetime'].dt.tz_localize(None)\n",
    "        if resolution == \"D\": data['Datetime'] = data['Datetime'].dt.normalize() + pd.Timedelta(hours=9, minutes=15)\n",
    "        data = data[['Datetime', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {symbol}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# data = download_stock_data('NSE:RELIANCE-EQ', \n",
    "#                            days=10, \n",
    "#                            resolution=\"D\", \n",
    "#                            range_from=\"2025-09-01\", \n",
    "#                            range_to=\"2025-09-30\")\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- Helper Functions -----------------------------------\n",
    "\n",
    "def calculate_expected_candles(resolution, trading_days):\n",
    "    \"\"\"\n",
    "    Calculate expected number of candles for given resolution and trading days.\n",
    "    Market hours: 9:15 AM to 3:30 PM = 375 minutes = 22,500 seconds\n",
    "    \"\"\"\n",
    "    total_seconds = 22500  # 6h 15min in seconds\n",
    "\n",
    "    resolution_map = {\n",
    "        \"5S\": 5, \"10S\": 10, \"15S\": 15, \"30S\": 30, \"45S\": 45,                   # Seconds based\n",
    "        \"1\": 60, \"2\": 120, \"3\": 180, \"5\": 300, \"10\": 600, \"15\": 900,           # Minutes based\n",
    "        \"20\": 1200, \"30\": 1800, \"60\": 3600, \"120\": 7200, \"240\": 14400,\n",
    "        \"D\": 22500                                                              # Daily         \n",
    "    }\n",
    "\n",
    "    if resolution not in resolution_map:\n",
    "        raise ValueError(f\"Unsupported resolution: {resolution}\")\n",
    "\n",
    "    candles_per_day = total_seconds // resolution_map[resolution]\n",
    "    return trading_days * candles_per_day\n",
    "\n",
    "\n",
    "def get_trading_days(start_date, end_date, stock_name=None, reference_csv=\"D:/Programming/Download_Backtest_Deploy_data/1__Download/TradingView_data_download/NIFTY_50_TV_D.csv\"):\n",
    "\n",
    "    LISTING_DATES = {\n",
    "        \"NSE:ITCHOTELS-EQ\": \"2025-01-29\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(reference_csv)\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "        start = pd.to_datetime(start_date)\n",
    "        end = pd.to_datetime(end_date)\n",
    "        \n",
    "        # Apply listing date filter if available\n",
    "        if stock_name and stock_name in LISTING_DATES:\n",
    "            listing_date = pd.to_datetime(LISTING_DATES[stock_name])\n",
    "            start = max(start, listing_date)  # Use later date\n",
    "\n",
    "        mask = (df['Datetime'].dt.date >= start.date()) & (df['Datetime'].dt.date <= end.date())\n",
    "        trading_days = df.loc[mask, 'Datetime'].dt.date.tolist()\n",
    "\n",
    "        return [str(day) for day in trading_days]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading trading days: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def verify_data_completeness(data, expected_candles, trading_days, resolution, threshold=0.999):\n",
    "    \"\"\"\n",
    "    ENHANCED: Verify downloaded data completeness with per-day candle count check.\n",
    "    Returns: dict with verification results including incomplete_days\n",
    "    \"\"\"\n",
    "    if data is None or data.empty:\n",
    "        return {\n",
    "            \"status\": \"failed\",\n",
    "            \"actual_candles\": 0,\n",
    "            \"expected_candles\": expected_candles,\n",
    "            \"missing_days\": trading_days,\n",
    "            \"incomplete_days\": [],\n",
    "            \"completeness_pct\": 0.0\n",
    "        }\n",
    "\n",
    "    actual_candles = len(data)\n",
    "    data['Date'] = pd.to_datetime(data['Datetime']).dt.date.astype(str)\n",
    "    available_days = data['Date'].unique().tolist()\n",
    "\n",
    "    # Check for completely missing days\n",
    "    missing_days = [day for day in trading_days if day not in available_days]\n",
    "\n",
    "    # Check ALL days for missing candles (not just incomplete ones)\n",
    "    incomplete_days = []\n",
    "    missing_candles_details = []\n",
    "    expected_per_day = calculate_expected_candles(resolution, 1)\n",
    "\n",
    "    if expected_per_day:\n",
    "        day_counts = data.groupby('Date').size()\n",
    "        \n",
    "        for day in trading_days:\n",
    "            if day not in missing_days:  # Only check days that exist\n",
    "                actual_day_candles = day_counts.get(day, 0)\n",
    "                expected_threshold = expected_per_day * threshold\n",
    "                \n",
    "                # Check if day is below threshold\n",
    "                if actual_day_candles < expected_threshold:\n",
    "                    incomplete_days.append(day)\n",
    "                \n",
    "                # NEW: Check for ANY missing candles (even if above threshold)\n",
    "                if actual_day_candles < expected_per_day:\n",
    "                    missing_count = expected_per_day - actual_day_candles\n",
    "                    \n",
    "                    # Generate expected timestamps\n",
    "                    day_data = data[data['Date'] == day]['Datetime'].sort_values()\n",
    "                    market_start = pd.to_datetime(f\"{day} 09:15:00\")\n",
    "                    market_end = pd.to_datetime(f\"{day} 15:29:55\")\n",
    "                    \n",
    "                    # Resolution mapping\n",
    "                    freq_map = {\n",
    "                        \"5S\": \"5S\", \"10S\": \"10S\", \"15S\": \"15S\", \"30S\": \"30S\",\n",
    "                        \"1\": \"1min\", \"2\": \"2min\", \"5\": \"5min\", \"15\": \"15min\", \"30\": \"30min\"\n",
    "                    }\n",
    "                    freq = freq_map.get(resolution, \"1min\")\n",
    "                    \n",
    "                    expected_timestamps = pd.date_range(start=market_start, end=market_end, freq=freq)\n",
    "                    actual_timestamps = pd.to_datetime(day_data)\n",
    "                    \n",
    "                    # Find missing timestamps\n",
    "                    missing_timestamps = expected_timestamps.difference(actual_timestamps)\n",
    "                    \n",
    "                    missing_candles_details.append({\n",
    "                        \"date\": day,\n",
    "                        \"missing_count\": len(missing_timestamps),\n",
    "                        \"actual_candles\": actual_day_candles,\n",
    "                        \"expected_candles\": expected_per_day,\n",
    "                        \"missing_timestamps\": missing_timestamps.strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "                    })\n",
    "\n",
    "    completeness_pct = (actual_candles / expected_candles * 100) if expected_candles > 0 else 0\n",
    "\n",
    "    total_problem_days = len(missing_days) + len(incomplete_days)\n",
    "    status = \"complete\" if (total_problem_days == 0 and completeness_pct >= 99.9) else \"incomplete\"\n",
    "\n",
    "    result = {\n",
    "        \"status\": status,\n",
    "        \"actual_candles\": actual_candles,\n",
    "        \"expected_candles\": expected_candles,\n",
    "        \"missing_days\": missing_days,\n",
    "        \"incomplete_days\": incomplete_days,\n",
    "        \"missing_candles_details\": missing_candles_details,\n",
    "        \"completeness_pct\": round(completeness_pct, 2),\n",
    "        \"expected_per_day\": expected_per_day\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_data_status():\n",
    "    \"\"\"Load status tracking JSON for all stocks\"\"\"\n",
    "    status_file = \"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/fyers_data_status.json\"\n",
    "    try:\n",
    "        if os.path.exists(status_file):\n",
    "            with open(status_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "    except:\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "\n",
    "def save_data_status(status_data):\n",
    "    \"\"\"Save status tracking JSON for all stocks\"\"\"\n",
    "    status_file = \"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/fyers_data_status.json\"\n",
    "    with open(status_file, 'w') as f:\n",
    "        json.dump(status_data, f, indent=2)\n",
    "\n",
    "\n",
    "def fill_problem_days(symbol, resolution, problem_days, existing_data=None):\n",
    "    \"\"\"\n",
    "    Download data for missing or incomplete days.\n",
    "    For incomplete days, replaces existing partial data with fresh download.\n",
    "    \n",
    "    Returns: DataFrame with data from problem days\n",
    "    \"\"\"\n",
    "    if not problem_days:\n",
    "        return None\n",
    "\n",
    "    print(f\"  → Downloading {len(problem_days)} problem days: {problem_days[:5]}{'...' if len(problem_days) > 5 else ''}\")\n",
    "\n",
    "    all_new_data = []\n",
    "\n",
    "    for day in problem_days:\n",
    "        print(symbol, 1, resolution, day, day)\n",
    "        data = download_stock_data(symbol, days=1, resolution=resolution, range_from=day, range_to=day)\n",
    "        if data is not None and not data.empty:\n",
    "            all_new_data.append(data)\n",
    "            print(f\"    ✓ {day}: {len(data)} candles\")\n",
    "        else:\n",
    "            print(f\"    ✗ {day}: No data\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    if all_new_data:\n",
    "        new_data = pd.concat(all_new_data, ignore_index=True)\n",
    "        \n",
    "        # If existing data provided, remove old data for these days before merging\n",
    "        if existing_data is not None and not existing_data.empty:\n",
    "            existing_data['Datetime'] = pd.to_datetime(existing_data['Datetime'])\n",
    "            new_data['Datetime'] = pd.to_datetime(new_data['Datetime'])\n",
    "            \n",
    "            existing_data['Date'] = pd.to_datetime(existing_data['Datetime']).dt.date.astype(str)\n",
    "            # Remove old data for problem days\n",
    "            existing_data = existing_data[~existing_data['Date'].isin(problem_days)].drop(columns=['Date'])\n",
    "            # Merge with new data\n",
    "            merged_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "            merged_data = merged_data.sort_values('Datetime').reset_index(drop=True)\n",
    "            return merged_data\n",
    "        else:\n",
    "            return new_data\n",
    "    \n",
    "    return existing_data if existing_data is not None else None\n",
    "\n",
    "\n",
    "def download_with_verification(symbol, resolution, range_from, range_to, days=10, retries=3):\n",
    "    \"\"\"\n",
    "    - Attempt 1: Download full range\n",
    "    - Attempt 2-3: Download ONLY missing/incomplete days and merge with existing data\n",
    "    \n",
    "    Returns: (data_df, verification_result, best_attempt)\n",
    "    \"\"\"\n",
    "    trading_days = get_trading_days(range_from, range_to, symbol)\n",
    "    expected_candles = calculate_expected_candles(resolution, len(trading_days))\n",
    "\n",
    "    current_data = None\n",
    "    current_verification = None\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(f\"Attempt {attempt + 1}/{retries} for {symbol} ({resolution})\")\n",
    "        print(f\"{'-'*60}\")\n",
    "\n",
    "        if attempt == 0:\n",
    "            # First attempt: Download full range\n",
    "            print(f\"Downloading full range: {range_from} to {range_to}\")\n",
    "            current_data = download_stock_data(symbol, days, resolution, range_from, range_to)\n",
    "            current_verification = verify_data_completeness(current_data, expected_candles, trading_days, resolution)\n",
    "            \n",
    "        else:\n",
    "            # Subsequent attempts: Download only missing/incomplete days\n",
    "            if current_verification:\n",
    "                problem_days = current_verification['missing_days'] + current_verification['incomplete_days']\n",
    "                \n",
    "                if len(problem_days) > 0:\n",
    "                    print(f\"Fixing {len(problem_days)} problem days:\")\n",
    "                    print(f\"  • Missing: {len(current_verification['missing_days'])} days\")\n",
    "                    print(f\"  • Incomplete: {len(current_verification['incomplete_days'])} days\")\n",
    "                    \n",
    "                    current_data = fill_problem_days(symbol, resolution, problem_days, current_data)\n",
    "                    \n",
    "                    if current_data is not None:\n",
    "                        print(f\"  → Total candles after merge: {len(current_data)}\")\n",
    "                    else:\n",
    "                        print(f\"  ✗ No data retrieved from API\")\n",
    "                    \n",
    "                    # Re-verify after merge\n",
    "                    current_verification = verify_data_completeness(current_data, expected_candles, trading_days, resolution)\n",
    "                else:\n",
    "                    print(\"  → No problem days to fix\")\n",
    "\n",
    "        # Display results\n",
    "        if current_verification:\n",
    "            print(f\"\\nResult: {current_verification['actual_candles']}/{current_verification['expected_candles']} candles ({current_verification['completeness_pct']}%)\")\n",
    "            print(f\"Status: {current_verification['status']}\")\n",
    "            print(f\"Missing days: {len(current_verification['missing_days'])}\")\n",
    "            print(f\"Incomplete days: {len(current_verification['incomplete_days'])}\")\n",
    "            \n",
    "            # If complete, return immediately\n",
    "            if current_verification['status'] == 'complete':\n",
    "                print(f\"✓ COMPLETE on attempt {attempt + 1}\")\n",
    "                return current_data, current_verification, {\"candles\": current_verification['actual_candles'], \"attempt\": attempt + 1}\n",
    "        else:\n",
    "            print(\"✗ Download failed\")\n",
    "\n",
    "        problem_count = 0\n",
    "        if current_verification:\n",
    "            problem_count = len(current_verification['missing_days']) + len(current_verification['incomplete_days'])\n",
    "        \n",
    "        if attempt < retries - 1 and problem_count > 0:\n",
    "            print(f\"\\n→ Retrying to fix {problem_count} remaining problem days...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # After all retries\n",
    "    if current_verification:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Final result after {retries} attempts:\")\n",
    "        print(f\"  Candles: {current_verification['actual_candles']}/{current_verification['expected_candles']}\")\n",
    "        print(f\"  Completeness: {current_verification['completeness_pct']}%\")\n",
    "        print(f\"  Missing days: {len(current_verification['missing_days'])}\")\n",
    "        print(f\"  Incomplete days: {len(current_verification['incomplete_days'])}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return current_data, current_verification, {\"candles\": current_verification['actual_candles'], \"attempt\": retries}\n",
    "    \n",
    "    # Complete failure\n",
    "    failed_verification = verify_data_completeness(None, expected_candles, trading_days, resolution)\n",
    "    return None, failed_verification, {\"candles\": 0, \"attempt\": retries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def master_download(resolution, days, range_from, range_to):\n",
    "\n",
    "    data_folder = f\"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/storage_Fyers_{resolution}\"\n",
    "\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "    status_data = load_data_status()\n",
    "\n",
    "    successful = []\n",
    "    failed = []\n",
    "    incomplete = []\n",
    "\n",
    "    print(f\"\\n{'='*120}\")\n",
    "    print(f\"{' FYERS DATA DOWNLOAD (ENHANCED) ':-^120}\")\n",
    "    print(f\"{'='*120}\")\n",
    "    print(f\"Resolution: {resolution}\")\n",
    "    print(f\"Date Range: {range_from} to {range_to}\")\n",
    "    print(f\"Total Stocks: {len(nifty_50)}\")\n",
    "    print(f\"{'='*120}\\n\")\n",
    "\n",
    "    for i, symbol in enumerate(nifty_50, 1):\n",
    "        print(i, symbol)\n",
    "\n",
    "        stock_name = symbol.replace(\"NSE:\", \"\").replace(\"-EQ\", \"\").replace(\"-INDEX\", \"\").replace('-', '_')\n",
    "        filename = f\"{data_folder}/{stock_name}_Fyers_{resolution}.csv\"\n",
    "        \n",
    "        print(f\"{'='*90}\")\n",
    "        print(f\"{f' [{i}/{len(nifty_50)}] {stock_name} ':-^90}\")\n",
    "\n",
    "        # Initialize stock status if not exists\n",
    "        if stock_name not in status_data:\n",
    "            status_data[stock_name] = {}\n",
    "        \n",
    "        if resolution not in status_data[stock_name]:\n",
    "            status_data[stock_name][resolution] = {}\n",
    "\n",
    "        # Check if file already exists\n",
    "        if os.path.exists(filename):\n",
    "            existing_data = pd.read_csv(filename)\n",
    "            print(f\"✓ Existing file found: {len(existing_data)} candles\")\n",
    "\n",
    "            # Verify existing data\n",
    "            trading_days = get_trading_days(range_from, range_to, symbol)\n",
    "            expected_candles = calculate_expected_candles(resolution, len(trading_days))\n",
    "            verification = verify_data_completeness(existing_data, expected_candles, trading_days, resolution)\n",
    "\n",
    "            # Printing details\n",
    "            print(f\"  Status: {verification['completeness_pct']}% complete\")\n",
    "            print(f\"  Candles: {verification['actual_candles']}/{verification['expected_candles']}\")\n",
    "            print(f\"  Missing days: {len(verification['missing_days'])}\")\n",
    "            print(f\"  Incomplete days: {len(verification['incomplete_days'])}\")\n",
    "            print(f\"  missing_candles_details:\")\n",
    "            for missing_candles_day in verification['missing_candles_details']:\n",
    "                print(f\"    Date: {missing_candles_day['date']} | Missing: {missing_candles_day['missing_count']} candles\")\n",
    "                print(f\"      Timestamps: {missing_candles_day['missing_timestamps'][:5]}\")  # First 5 timestamps\n",
    "\n",
    "            if verification['status'] == 'complete':\n",
    "                print(\"✓ Data is complete. Skipping.\\n\")\n",
    "                successful.append(stock_name)\n",
    "                status_data[stock_name][resolution] = {\n",
    "                    \"last_updated\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"date_range\": {\"from\": range_from, \"to\": range_to},\n",
    "                    \"trading_days_expected\": len(trading_days),\n",
    "                    \"expected_candles\": int(expected_candles),\n",
    "                    \"actual_candles\": int(verification['actual_candles']),\n",
    "                    \"missing_days\": [],\n",
    "                    \"incomplete_days\": [],\n",
    "                    \"missing_candles_details\": [],\n",
    "                    \"status\": \"complete\",\n",
    "                    \"completeness_pct\": 100.0\n",
    "                }\n",
    "                save_data_status(status_data)\n",
    "                continue\n",
    "            else:\n",
    "                problem_days = verification['missing_days'] + verification['incomplete_days']\n",
    "                print(f\"⚠ Total problem days: {len(problem_days)}\")\n",
    "                print(\"  → Will attempt to fix problem days...\\n\")\n",
    "\n",
    "                # Fix problem days\n",
    "                if len(problem_days) > 0:\n",
    "                    fixed_data = fill_problem_days(symbol, resolution, problem_days, existing_data)\n",
    "\n",
    "                    if fixed_data is not None and not fixed_data.empty:\n",
    "                        # Re-verify\n",
    "                        final_verification = verify_data_completeness(fixed_data, expected_candles, trading_days, resolution)\n",
    "                        \n",
    "                        fixed_data.to_csv(filename, index=False)\n",
    "                        \n",
    "                        # Update status\n",
    "                        status_data[stock_name][resolution] = {\n",
    "                            \"last_updated\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                            \"date_range\": {\"from\": range_from, \"to\": range_to},\n",
    "                            \"trading_days_expected\": len(trading_days),\n",
    "                            \"expected_candles\": int(expected_candles),\n",
    "                            \"actual_candles\": int(final_verification['actual_candles']),\n",
    "                            \"missing_days\": [str(d) for d in final_verification['missing_days']],\n",
    "                            \"incomplete_days\": [str(d) for d in final_verification['incomplete_days']],\n",
    "                            \"missing_candles_details\": [\n",
    "                                {\n",
    "                                    \"date\": str(item['date']),\n",
    "                                    \"missing_count\": int(item['missing_count']),\n",
    "                                    \"missing_timestamps\": \", \".join([str(ts) for ts in item['missing_timestamps']])\n",
    "                                }\n",
    "                                for item in final_verification['missing_candles_details']\n",
    "                            ],\n",
    "                            \"status\": final_verification['status'],\n",
    "                            \"completeness_pct\": float(final_verification['completeness_pct'])\n",
    "                        }\n",
    "\n",
    "                        save_data_status(status_data)\n",
    "                        \n",
    "                        if final_verification['status'] == 'complete':\n",
    "                            print(f\"✓ Successfully fixed all problem days!\")\n",
    "                            successful.append(stock_name)\n",
    "                        else:\n",
    "                            print(f\"⚠ Still incomplete: {final_verification['completeness_pct']}%\")\n",
    "                            incomplete.append(stock_name)\n",
    "                        \n",
    "                        continue\n",
    "\n",
    "        # Download with enhanced verification (per-day check + incremental retry)\n",
    "        data, verification, best_attempt = download_with_verification(\n",
    "            symbol, resolution, range_from, range_to, days=days, retries=3\n",
    "        )\n",
    "\n",
    "        # Update status\n",
    "        status_data[stock_name][resolution] = {\n",
    "            \"last_updated\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"date_range\": {\"from\": range_from, \"to\": range_to},\n",
    "            \"trading_days_expected\": len(get_trading_days(range_from, range_to)),\n",
    "            \"expected_candles\": int(verification['expected_candles']),\n",
    "            \"actual_candles\": int(verification['actual_candles']),\n",
    "            \"missing_days\": [str(d) for d in verification['missing_days']],\n",
    "            \"incomplete_days\": [str(d) for d in verification.get('incomplete_days', [])],\n",
    "            \"missing_candles_details\": [\n",
    "                {\n",
    "                    \"date\": str(item['date']),\n",
    "                    \"missing_count\": int(item['missing_count']),\n",
    "                    \"missing_timestamps\": \", \".join([str(ts) for ts in item['missing_timestamps']])\n",
    "                }\n",
    "                for item in verification.get('missing_candles_details', [])\n",
    "            ],\n",
    "            \"status\": verification['status'],\n",
    "            \"completeness_pct\": float(verification['completeness_pct']),\n",
    "            \"retry_count\": best_attempt['attempt'],\n",
    "            \"best_attempt\": best_attempt\n",
    "        }\n",
    "        \n",
    "        # Handle based on status\n",
    "        if verification['status'] == 'complete':\n",
    "            data.to_csv(filename, index=False)\n",
    "            successful.append(stock_name)\n",
    "            save_data_status(status_data)\n",
    "            print(f\"\\n✓ SUCCESS: {stock_name} saved\")\n",
    "            \n",
    "        elif verification['status'] == 'incomplete':\n",
    "            data.to_csv(filename, index=False)\n",
    "            incomplete.append(stock_name)\n",
    "            save_data_status(status_data)\n",
    "            \n",
    "            print(f\"\\n⚠ INCOMPLETE: {stock_name}\")\n",
    "            print(f\"  Completeness: {verification['completeness_pct']}%\")\n",
    "            print(f\"  Missing days: {len(verification['missing_days'])}\")\n",
    "            print(f\"  Incomplete days: {len(verification.get('incomplete_days', []))}\")\n",
    "            print(f\"  missing_candles_details:\")\n",
    "            for missing_candles_day in verification['missing_candles_details']:\n",
    "                print(f\"    Date: {missing_candles_day['date']} | Missing: {missing_candles_day['missing_count']} candles\")\n",
    "                print(f\"      Timestamps: {missing_candles_day['missing_timestamps'][:5]}\")  # First 5 timestamps\n",
    "\n",
    "            print(f\"✓ Using available data: {verification['actual_candles']} candles\")\n",
    "            status_data[stock_name][resolution]['user_action'] = 'used_best_data'\n",
    "            save_data_status(status_data)\n",
    "            \n",
    "        else:\n",
    "            failed.append(stock_name)\n",
    "            print(f\"\\n✗ FAILED: {stock_name}\")\n",
    "            save_data_status(status_data)\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Final Summary\n",
    "    print(f\"\\n{'='*120}\")\n",
    "    print(f\"{'-- DOWNLOAD SUMMARY --':-^120}\")\n",
    "    print(f\"{'='*120}\")\n",
    "    print(f\"✓ Successful: {len(successful)}/{len(nifty_50)}\")\n",
    "    print(f\"⚠ Incomplete: {len(incomplete)}/{len(nifty_50)}\")\n",
    "    print(f\"✗ Failed: {len(failed)}/{len(nifty_50)}\")\n",
    "\n",
    "    # Calculate aggregate statistics\n",
    "    total_expected = 0\n",
    "    total_actual = 0\n",
    "    total_missing_days = 0\n",
    "    total_incomplete_days = 0\n",
    "    total_missing_candles = 0\n",
    "\n",
    "    for stock in nifty_50:\n",
    "        stock_name = stock.replace(\"NSE:\", \"\").replace(\"-EQ\", \"\").replace(\"-INDEX\", \"\").replace('-', '_')\n",
    "        if stock_name in status_data and resolution in status_data[stock_name]:\n",
    "            stock_status = status_data[stock_name][resolution]\n",
    "            total_expected += stock_status.get('expected_candles', 0)\n",
    "            total_actual += stock_status.get('actual_candles', 0)\n",
    "            total_missing_days += len(stock_status.get('missing_days', []))\n",
    "            total_incomplete_days += len(stock_status.get('incomplete_days', []))\n",
    "\n",
    "    total_missing_candles = total_expected - total_actual\n",
    "\n",
    "    print(f\"\\n{'--- AGGREGATE STATISTICS ---':-^60}\")\n",
    "    print(f\"Total Expected Candles: {total_expected:,}\")\n",
    "    print(f\"Total Actual Candles: {total_actual:,}\")\n",
    "    print(f\"Total Missing Candles: {total_missing_candles:,}\")\n",
    "    print(f\"Overall Completeness: {(total_actual/total_expected*100):.2f}%\" if total_expected > 0 else \"N/A\")\n",
    "    print(f\"Total Missing Days: {total_missing_days}\")\n",
    "    print(f\"Total Incomplete Days: {total_incomplete_days}\")\n",
    "\n",
    "    if successful:\n",
    "        print(f\"\\n✓ Complete stocks ({len(successful)}):\")\n",
    "        for stock in successful[:10]:\n",
    "            print(f\"  • {stock}\")\n",
    "        if len(successful) > 10:\n",
    "            print(f\"  ... and {len(successful) - 10} more\")\n",
    "\n",
    "    if incomplete:\n",
    "        print(f\"\\n⚠ Incomplete stocks ({len(incomplete)}):\")\n",
    "        for stock in incomplete:\n",
    "            stock_info = status_data.get(stock, {}).get(resolution, {})\n",
    "            completeness = stock_info.get('completeness_pct', 0)\n",
    "            print(f\"  • {stock} ({completeness}%)\")\n",
    "\n",
    "    if failed:\n",
    "        print(f\"\\n✗ Failed stocks ({len(failed)}):\")\n",
    "        for stock in failed:\n",
    "            print(f\"  • {stock}\")\n",
    "    \n",
    "    print(f\"\\n{'='*120}\\n\")\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "    return successful, incomplete, failed\n",
    "\n",
    "# =======================================================\n",
    "\n",
    "resolution = \"5S\"\n",
    "\n",
    "# Test master download with all stocks\n",
    "successful, incomplete, failed = master_download(resolution=resolution,\n",
    "                                                 days=100, \n",
    "                                                 range_from=\"2025-01-01\",  \n",
    "                                                 range_to=\"2025-09-30\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load status file\n",
    "with open(\"D:/Programming/Download_Backtest_Deploy_data/1__Download/1__Download_data_Fyers_via_API/fyers_data_status.json\", 'r') as f:\n",
    "    status_data = json.load(f)\n",
    "\n",
    "# Generate all expected timestamps for resolution\n",
    "market_start = pd.to_datetime(\"09:15:00\", format=\"%H:%M:%S\")\n",
    "market_end = pd.to_datetime(\"15:29:55\", format=\"%H:%M:%S\")\n",
    "all_timestamps = pd.date_range(start=market_start, end=market_end, freq=resolution)\n",
    "all_times = [ts.strftime('%H:%M:%S') for ts in all_timestamps]\n",
    "\n",
    "print(f\"Total expected timestamps: {len(all_times)}\")\n",
    "\n",
    "# Aggregate missing candles by timestamp across ALL stocks\n",
    "timestamp_counts = {time: 0 for time in all_times}  # Initialize all to 0\n",
    "\n",
    "for stock_name, resolutions in status_data.items():\n",
    "    if resolution in resolutions:\n",
    "        details = resolutions[resolution]\n",
    "        missing_details = details.get('missing_candles_details', [])\n",
    "        \n",
    "        for day_detail in missing_details:\n",
    "            timestamps = day_detail.get('missing_timestamps', [])\n",
    "            \n",
    "            # Handle both formats: list (new) or string (old)\n",
    "            if isinstance(timestamps, str):\n",
    "                # Old format: parse comma-separated string\n",
    "                timestamps = [ts.strip() for ts in timestamps.split(',') if '...' not in ts]\n",
    "            \n",
    "            # Now timestamps is always a list\n",
    "            for ts in timestamps:\n",
    "                if not ts:\n",
    "                    print(\"i am\", ts)\n",
    "                    continue\n",
    "                time_only = ts.split(' ')[1] if ' ' in ts else ts\n",
    "                if time_only in timestamp_counts:\n",
    "                    timestamp_counts[time_only] += 1\n",
    "                    \n",
    "# Display aggregated results\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"{f'MISSING CANDLES BY TIMESTAMP (ALL {len(all_times)} TIMESTAMPS)':^70}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Resolution: {resolution}\")\n",
    "print(f\"Total timestamps in trading day: {len(all_times)}\")\n",
    "print(f\"\\n{'Time':<9} {'Missing Count':<10}\")\n",
    "print(f\"{'-'*70}\")\n",
    "\n",
    "total_missing = sum(timestamp_counts.values())\n",
    "total_stocks = len(status_data)\n",
    "\n",
    "# Display ALL timestamps\n",
    "time_list = []\n",
    "for time in all_times:\n",
    "    time_list.append(time)\n",
    "\n",
    "    if len(time_list) == 5:\n",
    "        for t in time_list:\n",
    "            count = timestamp_counts[t]\n",
    "            print(f\"{t:<9} {count:<4} {'Missing' if count > 0 else '       '}\", end='    ')\n",
    "\n",
    "        time_list = []\n",
    "        print()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Summary:\")\n",
    "print(f\"  Total stocks analyzed: {total_stocks}\")\n",
    "print(f\"  Total timestamps: {len(all_times)}\")\n",
    "print(f\"  Timestamps with missing data: {sum(1 for c in timestamp_counts.values() if c > 0)}\")\n",
    "print(f\"  Most problematic time: {max(timestamp_counts.items(), key=lambda x: x[1])[0]} ({max(timestamp_counts.values())} missing)\")\n",
    "print(f\"  Total missing candles (all stocks, all times): {total_missing}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
